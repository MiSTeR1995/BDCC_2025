{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8d5e05-042a-47ea-ac9d-731db6bcb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPProcessor\n",
    "from models.models import EmotionMamba, PersonalityMamba, FusionTransformer\n",
    "from data_loading.feature_extractor import PretrainedImageEmbeddingExtractor\n",
    "from utils.config_loader import ConfigLoader\n",
    "\n",
    "def draw_box(image, box, color=(255, 0, 255)):\n",
    "    \"\"\"Draw a rectangle on the image.\"\"\"\n",
    "    line_width = 2\n",
    "    lw = line_width or max(round(sum(image.shape) / 2 * 0.003), 2)\n",
    "    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)\n",
    "\n",
    "def image_processing(image, image_processor):\n",
    "    image = image_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "    image = image['pixel_values']\n",
    "    return image\n",
    "\n",
    "def preprocess_face(face_roi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Предобработка области лица (пример: нормализация + resize).\"\"\"\n",
    "    # Пример: преобразуем в 112x112 и нормализуем [0, 1]\n",
    "    face_roi = cv2.resize(face_roi, (112, 112))\n",
    "    face_roi = face_roi.astype('float32') / 255.0\n",
    "    return face_roi\n",
    "\n",
    "def preprocess_body(body_roi: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Предобработка области тела (пример: нормализация + resize).\"\"\"\n",
    "    # Пример: преобразуем в 224x224 и нормализуем [0, 1]\n",
    "    body_roi = cv2.resize(body_roi, (224, 224))\n",
    "    body_roi = body_roi.astype('float32') / 255.0\n",
    "    return body_roi\n",
    "\n",
    "def select_uniform_frames(frames, N):\n",
    "    if len(frames) <= N:\n",
    "        return frames\n",
    "    else:\n",
    "        indices = np.linspace(0, len(frames) - 1, num=N, dtype=int)\n",
    "        return [frames[i] for i in indices]\n",
    "\n",
    "def get_fusion_model(config, device):\n",
    "    emo_model = EmotionMamba(\n",
    "    input_dim_emotion     = config.image_embedding_dim,\n",
    "    input_dim_personality = config.image_embedding_dim,\n",
    "    len_seq               = config.counter_need_frames, \n",
    "    hidden_dim            = config.hidden_dim_emo,\n",
    "    out_features          = config.out_features_emo,\n",
    "    tr_layer_number       = config.tr_layer_number_emo,\n",
    "    num_transformer_heads = config.num_transformer_heads_emo,\n",
    "    positional_encoding   = config.positional_encoding_emo,\n",
    "    mamba_d_model         = config.mamba_d_state_emo,\n",
    "    mamba_layer_number    = config.mamba_layer_number_emo,\n",
    "    dropout               = config.dropout,\n",
    "    num_emotions          = 7,\n",
    "    num_traits            = 5,\n",
    "    device                = device\n",
    "    ).to(device).eval()\n",
    "    # параметры задаем для лучшей персональной модели\n",
    "    per_model = PersonalityMamba(\n",
    "    input_dim_emotion     = config.image_embedding_dim,\n",
    "    input_dim_personality = config.image_embedding_dim,\n",
    "    len_seq               = config.counter_need_frames, \n",
    "    hidden_dim            = config.hidden_dim_per,\n",
    "    out_features          = config.out_features_per,\n",
    "    per_activation        = config.best_per_activation,\n",
    "    tr_layer_number       = config.tr_layer_number_per,\n",
    "    num_transformer_heads = config.num_transformer_heads_per,\n",
    "    positional_encoding   = config.positional_encoding_per,\n",
    "    mamba_d_model         = config.mamba_d_state_per,\n",
    "    mamba_layer_number    = config.mamba_layer_number_per,\n",
    "    dropout               = config.dropout,\n",
    "    num_emotions          = 7,\n",
    "    num_traits            = 5,\n",
    "    device                = device\n",
    "    ).to(device).eval()\n",
    "\n",
    "    # emo_state = torch.load(config.path_to_saved_emotion_model, map_location=device)\n",
    "    # emo_model.load_state_dict(emo_state)\n",
    "\n",
    "    # emo_state = torch.load(config.path_to_saved_personality_model, map_location=device)\n",
    "    # per_model.load_state_dict(emo_state)\n",
    "    model = FusionTransformer(\n",
    "        emo_model             = emo_model,\n",
    "        per_model             = per_model,\n",
    "        input_dim_emotion     = config.image_embedding_dim,\n",
    "        input_dim_personality = config.image_embedding_dim,\n",
    "        hidden_dim            = config.hidden_dim,\n",
    "        out_features          = config.out_features,\n",
    "        per_activation        = config.per_activation,\n",
    "        tr_layer_number       = config.tr_layer_number,\n",
    "        num_transformer_heads = config.num_transformer_heads,\n",
    "        positional_encoding   = config.positional_encoding,\n",
    "        mamba_d_model         = config.mamba_d_state,\n",
    "        mamba_layer_number    = config.mamba_layer_number,\n",
    "        dropout               = config.dropout,\n",
    "        num_emotions          = 7,\n",
    "        num_traits            = 5,\n",
    "        device                = device\n",
    "        ).to(device).eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def transform_matrix(matrix):\n",
    "    threshold1 = 1 - 1/7 \n",
    "    threshold2 = 1/7\n",
    "    mask1 = matrix[:, 0] >= threshold1\n",
    "    result = np.zeros_like(matrix[:, 1:])\n",
    "    transformed = (matrix[:, 1:] >= threshold2).astype(int)\n",
    "    result[~mask1] = transformed[~mask1]\n",
    "    return result\n",
    "\n",
    "def process_predictions(pred_emo):\n",
    "    pred_emo = torch.nn.functional.softmax(pred_emo, dim=1).cpu().detach().numpy()\n",
    "    pred_emo = transform_matrix(pred_emo).tolist()\n",
    "    return pred_emo\n",
    "\n",
    "def get_metadata(video_path: str, segment_length: int, image_processor: None, image_feature_extractor: None, device: None) -> pd.DataFrame:\n",
    "    \"\"\"Основная функция: получает метаданные для видео.\"\"\"\n",
    "    if hasattr(body_detector.predictor, 'trackers'):\n",
    "        body_detector.predictor.trackers[0].reset()\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.basename(video_path)\n",
    "    w, h, fps, total_frames = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS, cv2.CAP_PROP_FRAME_COUNT))\n",
    "    need_frames = select_uniform_frames(list(range(total_frames)), segment_length)\n",
    "    \n",
    "    counter = 0\n",
    "    embeds = []\n",
    "\n",
    "    body_list = []\n",
    "    face_list = []\n",
    "    \n",
    "    while True:\n",
    "        ret, im0 = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if counter in need_frames:\n",
    "            # Детекция всех лиц\n",
    "            preprocessed_body = []\n",
    "            preprocessed_face = []\n",
    "            face_results = face_detector.process(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))\n",
    "            # Детекция всех тел\n",
    "            body_results = body_detector.track(im0, persist=True, imgsz=640, conf=0.01, iou=0.5, \n",
    "                                             augment=False, device=0, verbose=False)\n",
    "\n",
    "            # Случай 1: Есть лица — обрабатываем каждое\n",
    "            if face_results.detections:\n",
    "                for face_idx, detection in enumerate(face_results.detections):\n",
    "                    # Координаты лица\n",
    "                    bbox = detection.location_data.relative_bounding_box\n",
    "                    x1, y1 = max(int(bbox.xmin * w), 0), max(int(bbox.ymin * h), 0)\n",
    "                    x2, y2 = min(int((bbox.xmin + bbox.width) * w), w), min(int((bbox.ymin + bbox.height) * h), h)\n",
    "                    face_bbox = (x1, y1, x2, y2)\n",
    "                    face_center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "                    # Ищем тело, содержащее центр лица\n",
    "                    body_bbox = None\n",
    "                    body_id = -1\n",
    "                    if body_results and len(body_results[0].boxes) > 0:\n",
    "                        for box in body_results[0].boxes:\n",
    "                            box_coords = box.xyxy.int().cpu().numpy()[0]\n",
    "                            if (box_coords[0] <= face_center[0] <= box_coords[2] and \n",
    "                                box_coords[1] <= face_center[1] <= box_coords[3]):\n",
    "                                body_bbox = box_coords\n",
    "                                body_id = box.id.int().cpu().item() if box.id else -1\n",
    "                                break\n",
    "\n",
    "                    # Предобработка\n",
    "                    face_roi = im0[y1:y2, x1:x2]\n",
    "                    draw_box(im0, [x1, y1, x2, y2])\n",
    "                    draw_box(im0, [body_bbox[0], body_bbox[1], body_bbox[2], body_bbox[3]])\n",
    "                    preprocessed_face = image_processing(face_roi, image_processor) if face_roi.size > 0 else None\n",
    "                    \n",
    "                    if body_bbox is not None:\n",
    "                        body_roi = im0[body_bbox[1]:body_bbox[3], body_bbox[0]:body_bbox[2]]\n",
    "                        preprocessed_body = image_processing(body_roi, image_processor) if body_roi.size > 0 else None\n",
    "                    else:\n",
    "                        preprocessed_body = []\n",
    "\n",
    "                    # Сохраняем результат\n",
    "                    embeds.append([\n",
    "                        video_name, counter, body_id,\n",
    "                        x1, y1, x2, y2,\n",
    "                        body_bbox[0] if body_bbox is not None else None,\n",
    "                        body_bbox[1] if body_bbox is not None else None,\n",
    "                        body_bbox[2] if body_bbox is not None else None,\n",
    "                        body_bbox[3] if body_bbox is not None else None,\n",
    "                        # preprocessed_face,\n",
    "                        # preprocessed_body\n",
    "                    ])\n",
    "                    # print(preprocessed_body.shape)\n",
    "                    # print(preprocessed_face.shape)\n",
    "                    if preprocessed_body.shape[0] > 0:\n",
    "                        body_list.append(preprocessed_body)\n",
    "                    if preprocessed_face.shape[0] > 0:\n",
    "                        face_list.append(preprocessed_face)\n",
    "                    \n",
    "\n",
    "            # Случай 2: Лиц нет — берём самое большое тело\n",
    "            elif body_results and len(body_results[0].boxes) > 0:\n",
    "                largest_body = max(\n",
    "                    body_results[0].boxes,\n",
    "                    key=lambda box: (box.xyxy[0,2] - box.xyxy[0,0]) * (box.xyxy[0,3] - box.xyxy[0,1])\n",
    "                )\n",
    "                body_coords = largest_body.xyxy.int().cpu().numpy()[0]\n",
    "                body_id = largest_body.id.int().cpu().item() if largest_body.id else -1\n",
    "\n",
    "                # Предобработка тела\n",
    "                body_roi = im0[body_coords[1]:body_coords[3], body_coords[0]:body_coords[2]]\n",
    "                preprocessed_body = preprocess_body(body_roi) if body_roi.size > 0 else []\n",
    "\n",
    "                embeds.append([\n",
    "                    video_name, counter, body_id,\n",
    "                    None, None, None, None,  # Нет лица\n",
    "                    body_coords[0], body_coords[1], body_coords[2], body_coords[3],\n",
    "                    # None,  # Нет лица\n",
    "                    # preprocessed_body\n",
    "                ])\n",
    "\n",
    "                if preprocessed_body.shape[0] > 0:\n",
    "                    body_list.append(preprocessed_body)\n",
    "                if preprocessed_face.shape[0] > 0:\n",
    "                    face_list.append(preprocessed_face)\n",
    "\n",
    "            plt.imshow(cv2.cvtColor(im0, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "\n",
    "        counter += 1\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    body_list = torch.cat(body_list, dim=0)\n",
    "    body_feature = image_feature_extractor.extract(body_list).to(device)\n",
    "\n",
    "    face_list = torch.cat(face_list, dim=0)\n",
    "    face_feature = image_feature_extractor.extract(face_list).to(device)\n",
    "    \n",
    "    df = pd.DataFrame(embeds, columns=[\n",
    "        \"video_name\", \"frame\", \"person_id\",\n",
    "        \"face_x1\", \"face_y1\", \"face_x2\", \"face_y2\",\n",
    "        \"body_x1\", \"body_y1\", \"body_x2\", \"body_y2\",\n",
    "        # \"preprocessed_face\", \"preprocessed_body\"\n",
    "    ])\n",
    "    return df, body_feature, face_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca879103-6ed9-4998-9d67-2261ff0c4e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FusionTransformer:\n\tsize mismatch for emo_proj.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emo_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.0.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for per_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for emotion_personality_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for personality_emotion_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m config_face \u001b[38;5;241m=\u001b[39m ConfigLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_config_face.toml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# image_feature_extractor = PretrainedImageEmbeddingExtractor(config_body)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m image_feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedImageEmbeddingExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Models can download from https://drive.google.com/drive/folders/1APMtC4LXjuW9behd2TxVXz0DsjQKAgRR?usp=sharing\u001b[39;00m\n\u001b[0;32m     11\u001b[0m body_model \u001b[38;5;241m=\u001b[39m get_fusion_model(config_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Prgrm\\EAAI_2025\\data_loading\\feature_extractor.py:16\u001b[0m, in \u001b[0;36mPretrainedImageEmbeddingExtractor.__init__\u001b[1;34m(self, device, clip_name)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m CLIPProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(clip_name)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_fusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_model \u001b[38;5;241m=\u001b[39m get_fusion_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "File \u001b[1;32mC:\\Prgrm\\EAAI_2025\\utils\\body\\model_loader.py:103\u001b[0m, in \u001b[0;36mget_fusion_model\u001b[1;34m(modality, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# ←––– загружаем жёстко заданный checkpoint –––→\u001b[39;00m\n\u001b[0;32m    102\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(meta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m], map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# strict=True\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Prgrm\\EAAI_2025\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FusionTransformer:\n\tsize mismatch for emo_proj.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emo_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emo_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.0.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for per_proj.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for per_proj.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for emotion_to_personality_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for emotion_to_personality_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_weight: copying a param with shape torch.Size([3072, 1024]) from checkpoint, the shape in current model is torch.Size([768, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.in_proj_bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.self_attention.out_proj.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for personality_to_emotion_attn.0.feed_forward.layer_2.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_attention.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.add_norm_after_ff.norm.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for personality_to_emotion_attn.0.positional_encoding.pe: copying a param with shape torch.Size([5000, 1024]) from checkpoint, the shape in current model is torch.Size([5000, 256]).\n\tsize mismatch for emotion_personality_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for personality_emotion_fc_out.0.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([128, 512])."
     ]
    }
   ],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "body_detector = YOLO('extractors/body/best.pt')\n",
    "image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "config_body = ConfigLoader(\"inference_config_body.toml\")\n",
    "config_face = ConfigLoader(\"inference_config_face.toml\")\n",
    "# image_feature_extractor = PretrainedImageEmbeddingExtractor(config_body)\n",
    "image_feature_extractor = PretrainedImageEmbeddingExtractor(device=\"cuda\")\n",
    "# Models can download from https://drive.google.com/drive/folders/1APMtC4LXjuW9behd2TxVXz0DsjQKAgRR?usp=sharing\n",
    "\n",
    "body_model = get_fusion_model(config_body, 'cuda')\n",
    "face_model = get_fusion_model(config_face, 'cuda')\n",
    "# results_clip_body_true_mamba_fusiontransformer_2025-06-27_16-10-57/metrics_by_epoch/metrics_epochlog_FusionTransformer_num_transformer_heads_16_20250627_183039_timestamp/best_model_dev.pt\n",
    "body_fusion_model_path = 'extractors/body/clip_body_mamba_transformer_fusion_model.pt'\n",
    "# results_fusiontransformer_2025-07-03_09-41-13/metrics_by_epoch/metrics_epochlog_FusionTransformer_tr_layer_number_3_20250703_124848_timestamp/best_model_dev.pt\n",
    "face_fusion_model_path = 'extractors/face/clip_face_mamba_transformer_fusion_model.pt'\n",
    "\n",
    "body_state = torch.load(body_fusion_model_path, map_location='cuda')\n",
    "body_model.load_state_dict(body_state)\n",
    "\n",
    "face_state = torch.load(face_fusion_model_path, map_location='cuda')\n",
    "face_model.load_state_dict(face_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ac5f2-8c4a-4f4e-b95c-209089183698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bfabde07-afb4-4f68-9c28-4ba802d5921f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип верхнего объекта: <class 'dict'>\n",
      "\n",
      "----- HEADER -----\n",
      "mod: face\n",
      "extractor_fp: clipv:openai/clip-vit-base-patch32\n",
      "avg: True\n",
      "frames: 30\n",
      "img: 224\n",
      "text_col: None\n",
      "pre_v: v1\n",
      "\n",
      "----- DATA -----\n",
      "Количество сэмплов: 4\n",
      "Пример ключей: ['--qXJuDtHPw_23.1990_30.3250', '-571d8cVauQ_0.0000_4.9920', '-571d8cVauQ_21.8420_28.3090', '-571d8cVauQ_69.8930_85.0000']\n",
      "\n",
      "Первый ключ: --qXJuDtHPw_23.1990_30.3250\n",
      "mean: тензор формы torch.Size([512])\n",
      "  первые значения: tensor([-0.0106, -0.0707, -0.3738,  0.1331,  0.1042, -0.0987,  0.3389,  0.4817,\n",
      "         0.1298,  0.3130])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# путь к твоему файлу .pt\n",
    "pt_path = r\"C:/Prgrm/ICLR26/features/cmu_mosei/dev/face/face__clipv-openai-clip-vit-base-patch32__frames30__img224__avg1__pv-v1/feats_seed42_subset4_avg1.pt\"\n",
    "\n",
    "# загружаем через PyTorch\n",
    "data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"Тип верхнего объекта:\", type(data))\n",
    "\n",
    "# посмотрим header\n",
    "print(\"\\n----- HEADER -----\")\n",
    "if \"header\" in data:\n",
    "    for k, v in data[\"header\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# посмотрим data\n",
    "print(\"\\n----- DATA -----\")\n",
    "if \"data\" in data:\n",
    "    sample_keys = list(data[\"data\"].keys())\n",
    "    print(\"Количество сэмплов:\", len(sample_keys))\n",
    "    print(\"Пример ключей:\", sample_keys[:5])  # первые 5 ключей\n",
    "\n",
    "    # берём первый сэмпл\n",
    "    first_key = sample_keys[0]\n",
    "    print(\"\\nПервый ключ:\", first_key)\n",
    "\n",
    "    sample = data[\"data\"][first_key]\n",
    "\n",
    "    # показываем, что внутри (обычно mean / std)\n",
    "    for stat_name, tensor in sample.items():\n",
    "        if torch.is_tensor(tensor):\n",
    "            print(f\"{stat_name}: тензор формы {tensor.shape}\")\n",
    "            # первые 10 чисел для наглядности\n",
    "            print(\"  первые значения:\", tensor[:10])\n",
    "        else:\n",
    "            print(f\"{stat_name}: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30a46d35-c5a4-4697-b9d9-df728248dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип верхнего объекта: <class 'dict'>\n",
      "\n",
      "----- HEADER -----\n",
      "mod: audio\n",
      "extractor_fp: clapa:laion/clap-htsat-fused\n",
      "avg: mean\n",
      "frames: 0\n",
      "img: 0\n",
      "text_col: None\n",
      "pre_v: v1\n",
      "\n",
      "----- DATA -----\n",
      "Количество сэмплов: 1861\n",
      "Пример ключей: ['--qXJuDtHPw_23.1990_30.3250', '-571d8cVauQ_0.0000_4.9920', '-571d8cVauQ_21.8420_28.3090', '-571d8cVauQ_69.8930_85.0000', '-I_e4mIh0yE_16.8030_25.4695']\n",
      "\n",
      "Первый ключ: --qXJuDtHPw_23.1990_30.3250\n",
      "mean: тензор формы torch.Size([512])\n",
      "  первые значения: tensor([-0.0929, -0.0156,  0.0312, -0.0003,  0.0519, -0.0277, -0.0375, -0.1216,\n",
      "        -0.0494,  0.0514])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# путь к твоему файлу .pt\n",
    "pt_path = r\"C:/Prgrm/ICLR26/features/cmu_mosei/dev/audio/audio__clapa-laion-clap-htsat-fused__avg-mean_std__pv-v1/feats_seed42_subset0_avg-mean_std.pt\"\n",
    "\n",
    "# загружаем через PyTorch\n",
    "data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"Тип верхнего объекта:\", type(data))\n",
    "\n",
    "# посмотрим header\n",
    "print(\"\\n----- HEADER -----\")\n",
    "if \"header\" in data:\n",
    "    for k, v in data[\"header\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# посмотрим data\n",
    "print(\"\\n----- DATA -----\")\n",
    "if \"data\" in data:\n",
    "    sample_keys = list(data[\"data\"].keys())\n",
    "    print(\"Количество сэмплов:\", len(sample_keys))\n",
    "    print(\"Пример ключей:\", sample_keys[:5])  # первые 5 ключей\n",
    "\n",
    "    # берём первый сэмпл\n",
    "    first_key = sample_keys[0]\n",
    "    print(\"\\nПервый ключ:\", first_key)\n",
    "\n",
    "    sample = data[\"data\"][first_key]\n",
    "\n",
    "    # показываем, что внутри (обычно mean / std)\n",
    "    for stat_name, tensor in sample.items():\n",
    "        if torch.is_tensor(tensor):\n",
    "            print(f\"{stat_name}: тензор формы {tensor.shape}\")\n",
    "            # первые 10 чисел для наглядности\n",
    "            print(\"  первые значения:\", tensor[:10])\n",
    "        else:\n",
    "            print(f\"{stat_name}: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f3a05cd-35bb-46a4-8f95-194537f010a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип верхнего объекта: <class 'dict'>\n",
      "\n",
      "----- HEADER -----\n",
      "mod: audio\n",
      "extractor_fp: clapa:laion/clap-htsat-fused\n",
      "avg: mean_std\n",
      "frames: 0\n",
      "img: 0\n",
      "text_col: None\n",
      "pre_v: v1\n",
      "\n",
      "----- DATA -----\n",
      "Количество сэмплов: 5\n",
      "Пример ключей: ['--qXJuDtHPw_23.1990_30.3250', '-571d8cVauQ_0.0000_4.9920', '-571d8cVauQ_21.8420_28.3090', '-571d8cVauQ_69.8930_85.0000', '-I_e4mIh0yE_3.1440_10.5700']\n",
      "\n",
      "Первый ключ: --qXJuDtHPw_23.1990_30.3250\n",
      "mean: тензор формы torch.Size([768])\n",
      "  первые значения: tensor([ 0.5888, -0.0429,  0.1800, -0.2739, -0.7886,  0.7882,  0.0318,  0.1876,\n",
      "        -0.4165, -0.8684])\n",
      "std: тензор формы torch.Size([768])\n",
      "  первые значения: tensor([0.1291, 0.4231, 0.9009, 0.6434, 0.0283, 0.1644, 0.0821, 0.0370, 0.6012,\n",
      "        0.1113])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# путь к твоему файлу .pt\n",
    "pt_path = r\"C:/Prgrm/ICLR26/features/cmu_mosei/dev/audio/audio__clapa-laion-clap-htsat-fused__avg-mean_std__pv-v1/feats_seed42_subset5_avg-mean_std.pt\"\n",
    "\n",
    "# загружаем через PyTorch\n",
    "data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"Тип верхнего объекта:\", type(data))\n",
    "\n",
    "# посмотрим header\n",
    "print(\"\\n----- HEADER -----\")\n",
    "if \"header\" in data:\n",
    "    for k, v in data[\"header\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# посмотрим data\n",
    "print(\"\\n----- DATA -----\")\n",
    "if \"data\" in data:\n",
    "    sample_keys = list(data[\"data\"].keys())\n",
    "    print(\"Количество сэмплов:\", len(sample_keys))\n",
    "    print(\"Пример ключей:\", sample_keys[:5])  # первые 5 ключей\n",
    "\n",
    "    # берём первый сэмпл\n",
    "    first_key = sample_keys[0]\n",
    "    print(\"\\nПервый ключ:\", first_key)\n",
    "\n",
    "    sample = data[\"data\"][first_key]\n",
    "\n",
    "    # показываем, что внутри (обычно mean / std)\n",
    "    for stat_name, tensor in sample.items():\n",
    "        if torch.is_tensor(tensor):\n",
    "            print(f\"{stat_name}: тензор формы {tensor.shape}\")\n",
    "            # первые 10 чисел для наглядности\n",
    "            print(\"  первые значения:\", tensor[:10])\n",
    "        else:\n",
    "            print(f\"{stat_name}: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3288a33-40e3-468c-8290-9391bf2fd135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип верхнего объекта: <class 'dict'>\n",
      "\n",
      "----- HEADER -----\n",
      "mod: text\n",
      "extractor_fp: clapt:laion/clap-htsat-fused\n",
      "avg: True\n",
      "frames: 0\n",
      "img: 0\n",
      "text_col: None\n",
      "pre_v: v1\n",
      "\n",
      "----- DATA -----\n",
      "Количество сэмплов: 3\n",
      "Пример ключей: ['--qXJuDtHPw_23.1990_30.3250', '-571d8cVauQ_0.0000_4.9920', '-571d8cVauQ_21.8420_28.3090']\n",
      "\n",
      "Первый ключ: --qXJuDtHPw_23.1990_30.3250\n",
      "mean: тензор формы torch.Size([768])\n",
      "  первые значения: tensor([ 0.1183,  0.0298,  0.1684,  0.8365, -0.0719,  0.0729, -0.1034,  0.1622,\n",
      "        -0.3237, -0.6614])\n",
      "std: тензор формы torch.Size([768])\n",
      "  первые значения: tensor([0.3033, 0.1507, 0.1535, 0.4128, 0.3745, 0.2035, 0.1853, 0.4333, 0.2242,\n",
      "        0.2408])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# путь к твоему файлу .pt\n",
    "pt_path = r\"C:/Prgrm/ICLR26/features/cmu_mosei/dev/text/text__clapt-laion-clap-htsat-fused__avg1__pv-v1/feats_seed42_subset3_avg1.pt\"\n",
    "\n",
    "# загружаем через PyTorch\n",
    "data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"Тип верхнего объекта:\", type(data))\n",
    "\n",
    "# посмотрим header\n",
    "print(\"\\n----- HEADER -----\")\n",
    "if \"header\" in data:\n",
    "    for k, v in data[\"header\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# посмотрим data\n",
    "print(\"\\n----- DATA -----\")\n",
    "if \"data\" in data:\n",
    "    sample_keys = list(data[\"data\"].keys())\n",
    "    print(\"Количество сэмплов:\", len(sample_keys))\n",
    "    print(\"Пример ключей:\", sample_keys[:5])  # первые 5 ключей\n",
    "\n",
    "    # берём первый сэмпл\n",
    "    first_key = sample_keys[0]\n",
    "    print(\"\\nПервый ключ:\", first_key)\n",
    "\n",
    "    sample = data[\"data\"][first_key]\n",
    "\n",
    "    # показываем, что внутри (обычно mean / std)\n",
    "    for stat_name, tensor in sample.items():\n",
    "        if torch.is_tensor(tensor):\n",
    "            print(f\"{stat_name}: тензор формы {tensor.shape}\")\n",
    "            # первые 10 чисел для наглядности\n",
    "            print(\"  первые значения:\", tensor[:10])\n",
    "        else:\n",
    "            print(f\"{stat_name}: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9104b34f-7bc6-45ec-a7a8-45dd2ddeb8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип верхнего объекта: <class 'dict'>\n",
      "\n",
      "----- HEADER -----\n",
      "mod: behavior\n",
      "extractor_fp: clipt:openai/clip-vit-base-patch32\n",
      "avg: True\n",
      "frames: 0\n",
      "img: 0\n",
      "text_col: text_llm\n",
      "pre_v: v1\n",
      "\n",
      "----- DATA -----\n",
      "Количество сэмплов: 4\n",
      "Пример ключей: ['--qXJuDtHPw_23.1990_30.3250', '-571d8cVauQ_0.0000_4.9920', '-571d8cVauQ_21.8420_28.3090', '-571d8cVauQ_69.8930_85.0000']\n",
      "\n",
      "Первый ключ: --qXJuDtHPw_23.1990_30.3250\n",
      "mean: тензор формы torch.Size([512])\n",
      "  первые значения: tensor([ 0.0166,  0.1318,  0.1382, -0.2897, -0.2518, -0.0270,  0.0190, -0.1784,\n",
      "         0.0061,  0.0829])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# путь к твоему файлу .pt\n",
    "pt_path = r\"C:/Prgrm/ICLR26/features/cmu_mosei/dev/behavior/behavior__clipt-openai-clip-vit-base-patch32__col-text_llm__avg1__pv-v1/feats_seed42_subset4_avg1.pt\"\n",
    "\n",
    "# загружаем через PyTorch\n",
    "data = torch.load(pt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"Тип верхнего объекта:\", type(data))\n",
    "\n",
    "# посмотрим header\n",
    "print(\"\\n----- HEADER -----\")\n",
    "if \"header\" in data:\n",
    "    for k, v in data[\"header\"].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# посмотрим data\n",
    "print(\"\\n----- DATA -----\")\n",
    "if \"data\" in data:\n",
    "    sample_keys = list(data[\"data\"].keys())\n",
    "    print(\"Количество сэмплов:\", len(sample_keys))\n",
    "    print(\"Пример ключей:\", sample_keys[:5])  # первые 5 ключей\n",
    "\n",
    "    # берём первый сэмпл\n",
    "    first_key = sample_keys[0]\n",
    "    print(\"\\nПервый ключ:\", first_key)\n",
    "\n",
    "    sample = data[\"data\"][first_key]\n",
    "\n",
    "    # показываем, что внутри (обычно mean / std)\n",
    "    for stat_name, tensor in sample.items():\n",
    "        if torch.is_tensor(tensor):\n",
    "            print(f\"{stat_name}: тензор формы {tensor.shape}\")\n",
    "            # первые 10 чисел для наглядности\n",
    "            print(\"  первые значения:\", tensor[:10])\n",
    "        else:\n",
    "            print(f\"{stat_name}: {type(tensor)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8feb2-08f3-4773-8dd4-6eb8c1eaff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22918fe0-e230-4c7e-9918-3e7ab45dc910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80894920-6b37-461f-a6e7-26b6dd737749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип загруженного объекта: <class 'list'>\n",
      "\n",
      "list — первые элементы:\n",
      "[0] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_119.9190_125.2990', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_119.9190_125.2990.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_119.9190_125.2990.wav', 'labels': {'emotion': tensor([1., 0., 0., 0., 0., 0., 0.]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[1] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_13.6315_27.0310', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_13.6315_27.0310.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_13.6315_27.0310.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[2] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_27.0310_41.3000', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_27.0310_41.3000.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_27.0310_41.3000.wav', 'labels': {'emotion': tensor([0., 0., 0., 0., 1., 0., 0.]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[3] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_4.8400_13.6315', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_4.8400_13.6315.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_4.8400_13.6315.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.2000, 0.4000, 0.4000, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[4] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_82.7645_100.5550', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_82.7645_100.5550.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_82.7645_100.5550.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'C:/Prgrm/ICLR26/features/cmu_mosei/train/meta_seed42_subset5.pickle'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Тип загруженного объекта: {type(data)}\\n\")\n",
    "\n",
    "# Если это DataFrame\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    print(\"DataFrame — первые строки:\")\n",
    "    print(data.head())\n",
    "\n",
    "# Если это словарь\n",
    "elif isinstance(data, dict):\n",
    "    print(\"Словарь — первые ключи и значения:\")\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        print(f\"[{i}] Ключ: {key} — Тип значения: {type(value)}\")\n",
    "        print(f\"  Значение: {repr(value)[:1000]}\")\n",
    "        print(\"-\" * 80)\n",
    "        if i >= 4:\n",
    "            break\n",
    "\n",
    "# Если это список или кортеж\n",
    "elif isinstance(data, (list, tuple)):\n",
    "    print(f\"{type(data).__name__} — первые элементы:\")\n",
    "    for i, item in enumerate(data[:5]):\n",
    "        print(f\"[{i}] — Тип: {type(item)}\")\n",
    "        print(f\"  Значение: {repr(item)[:1000]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Всё остальное\n",
    "else:\n",
    "    print(\"Неизвестный тип. Показываю первые 1000 символов:\")\n",
    "    print(repr(data)[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab885c90-3605-45ac-8525-6c72d0967f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип загруженного объекта: <class 'list'>\n",
      "\n",
      "list — первые элементы:\n",
      "[0] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_119.9190_125.2990', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_119.9190_125.2990.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_119.9190_125.2990.wav', 'labels': {'emotion': tensor([1., 0., 0., 0., 0., 0., 0.]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[1] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_13.6315_27.0310', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_13.6315_27.0310.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_13.6315_27.0310.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[2] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_27.0310_41.3000', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_27.0310_41.3000.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_27.0310_41.3000.wav', 'labels': {'emotion': tensor([0., 0., 0., 0., 1., 0., 0.]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[3] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_4.8400_13.6315', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_4.8400_13.6315.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_4.8400_13.6315.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.2000, 0.4000, 0.4000, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n",
      "[4] — Тип: <class 'dict'>\n",
      "  Значение: {'sample_name': '-3g5yACwYnA_74.0830_82.7645', 'video_path': 'E:/CMU-MOSEI//video/train/-3g5yACwYnA_74.0830_82.7645.mp4', 'audio_path': 'E:/CMU-MOSEI//audio/train/-3g5yACwYnA_74.0830_82.7645.wav', 'labels': {'emotion': tensor([0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.6667, 0.0000]), 'personality': tensor([nan, nan, nan, nan, nan]), 'ah': tensor(nan)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'C:/Prgrm/ICLR26/features/cmu_mosei/train/meta_seed42_subset0.pickle'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"Тип загруженного объекта: {type(data)}\\n\")\n",
    "\n",
    "# Если это DataFrame\n",
    "if isinstance(data, pd.DataFrame):\n",
    "    print(\"DataFrame — первые строки:\")\n",
    "    print(data.head())\n",
    "\n",
    "# Если это словарь\n",
    "elif isinstance(data, dict):\n",
    "    print(\"Словарь — первые ключи и значения:\")\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        print(f\"[{i}] Ключ: {key} — Тип значения: {type(value)}\")\n",
    "        print(f\"  Значение: {repr(value)[:1000]}\")\n",
    "        print(\"-\" * 80)\n",
    "        if i >= 4:\n",
    "            break\n",
    "\n",
    "# Если это список или кортеж\n",
    "elif isinstance(data, (list, tuple)):\n",
    "    print(f\"{type(data).__name__} — первые элементы:\")\n",
    "    for i, item in enumerate(data[:5]):\n",
    "        print(f\"[{i}] — Тип: {type(item)}\")\n",
    "        print(f\"  Значение: {repr(item)[:1000]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Всё остальное\n",
    "else:\n",
    "    print(\"Неизвестный тип. Показываю первые 1000 символов:\")\n",
    "    print(repr(data)[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb5996-61e3-471b-b5e5-b7a21b8adad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d4d3641-6bec-401f-9595-fd3ea934b237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Шейпы признаков:\n",
      "\n",
      "[BODY]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([1024])\n",
      "  last_per_encoder_features: torch.Size([1024])\n",
      "\n",
      "[FACE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([512])\n",
      "  last_per_encoder_features: torch.Size([512])\n",
      "\n",
      "[SCENE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([768])\n",
      "  last_per_encoder_features: torch.Size([768])\n",
      "\n",
      "[AUDIO]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n",
      "\n",
      "[TEXT]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Дополнительно: распечатать шейпы признаков по модальностям\n",
    "print(\"\\n🔎 Шейпы признаков:\")\n",
    "modalities = item.get(\"features\", {})\n",
    "for mod_name, features in modalities.items():\n",
    "    print(f\"\\n[{mod_name.upper()}]\")\n",
    "    for feat_name, feat_val in features.items():\n",
    "        if isinstance(feat_val, torch.Tensor):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        elif isinstance(feat_val, np.ndarray):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {feat_name}: not a tensor ({type(feat_val)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3717095-7b61-4f31-bd8a-50d091ef1f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../features/cmu_mosei_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# путь к твоему .pickle файлу\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pickle_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../features/cmu_mosei_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpickle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Посмотреть первый элемент\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Prgrm\\EAAI_2025\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../features/cmu_mosei_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# путь к твоему .pickle файлу\n",
    "pickle_path = \"../features/cmu_mosei_test_seed_42_subset_size_2_average_features_False_feature_norm_False.pickle\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Посмотреть первый элемент\n",
    "print(\"🔍 Первый элемент:\")\n",
    "item = data[0]\n",
    "print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "808de167-db23-499e-acc4-2cf13f191a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Первый элемент:\n",
      "{'body': {'emotion_logits': tensor([-1.3183, -0.8238, -0.6552, -0.6306,  1.2407, -0.7158,  1.3891]), 'personality_scores': tensor([0.5326, 0.5365, 0.4849, 0.6321, 0.5423]), 'last_emo_encoder_features': tensor([-3.8351, -2.2751, -1.4746,  ..., -3.7185, -0.7212, -0.6948]), 'last_per_encoder_features': tensor([ 4.4531,  1.6310,  1.1421,  ...,  0.1990, -1.8134, -0.8094])}, 'face': {'emotion_logits': tensor([ 0.3950, -1.3291, -0.8733,  0.8395, -0.5555, -0.5297, -0.3164]), 'personality_scores': tensor([0.5448, 0.5832, 0.4877, 0.6371, 0.5482]), 'last_emo_encoder_features': tensor([-7.8037e+00,  7.3685e-01, -6.7197e-01,  5.2608e+00,  2.7093e+00,\n",
      "        -8.5908e-01,  7.0660e-01,  1.8818e+00, -7.1252e-02,  1.1847e-01,\n",
      "        -2.0084e+00,  6.3778e-01,  2.5741e+00, -1.2668e+00, -2.0097e+00,\n",
      "        -7.8275e+00,  3.3383e+00, -3.7198e+00, -1.6923e+00, -3.8017e-01,\n",
      "         8.0145e+00, -9.3273e-01, -6.2941e-01,  7.6822e-01,  3.5799e+00,\n",
      "        -5.1096e+00,  1.1017e+00, -4.3097e+00, -1.2193e+00,  1.5965e-01,\n",
      "         4.3750e+00, -4.2092e+00,  5.3175e+00,  3.1465e+00, -1.2244e+00,\n",
      "        -3.3327e+00,  3.1514e+00,  1.7549e+00, -1.4355e+00, -2.0142e-01,\n",
      "         3.6087e+00, -2.8974e-01, -3.7331e+00,  5.6680e-01, -3.0010e-01,\n",
      "         9.0169e-01, -1.1314e+00,  2.1992e+00,  4.5279e+00,  3.2596e+00,\n",
      "        -2.4183e+00, -5.7001e-01, -2.3132e+00, -3.1154e+00, -4.7663e+00,\n",
      "        -4.7279e+00, -2.4950e-01,  2.0279e+00,  2.3348e+00, -1.2379e+00,\n",
      "        -4.6050e-01, -1.1365e+00, -4.7051e-01,  1.8649e+00, -1.0845e+00,\n",
      "         1.6476e-01,  2.5328e+00,  2.3042e-01,  9.4413e-01,  2.4740e+00,\n",
      "        -2.4536e+00, -2.6942e-02, -5.6462e-02, -1.8618e+00, -2.1533e+00,\n",
      "        -4.3332e+00, -4.6510e+00, -5.3784e+00,  6.2951e-01,  3.2483e+00,\n",
      "        -1.5318e+00, -6.5289e+00, -6.8940e+00, -3.1748e+00, -1.0870e+00,\n",
      "        -7.6547e-01,  2.4738e-01, -2.9591e+00, -6.8369e-01,  5.6285e-01,\n",
      "        -4.0368e-01, -7.8555e+00,  3.7978e+00,  4.3003e+00, -2.6177e+00,\n",
      "        -1.7358e+00, -5.0570e-01, -3.5192e+00, -1.5536e+00, -1.7867e+00,\n",
      "        -2.7090e+00,  2.5931e-01, -2.4202e+00, -2.8526e+00,  1.5008e+00,\n",
      "        -3.1185e+00, -2.8423e+00, -3.7828e+00,  5.7870e+00, -5.9704e-01,\n",
      "        -1.8139e+00,  3.0643e+00, -5.1459e-01,  3.9783e+00,  5.7399e-01,\n",
      "         6.6687e-01, -2.8387e+00,  3.2814e+00, -2.7258e-01, -7.9078e-01,\n",
      "        -3.1760e+00,  8.4164e-02, -6.7941e-01,  3.7919e+00, -2.5553e+00,\n",
      "        -8.8240e-01, -4.1269e-01, -2.6904e+00,  2.4317e+00,  8.0473e-01,\n",
      "         1.3077e-01, -1.2166e+00,  1.9612e+00, -2.4235e+00, -3.1416e+00,\n",
      "        -3.2562e+00, -6.5874e-03, -1.6928e+00, -5.3513e+00, -3.8961e+00,\n",
      "         6.4773e+00,  2.3473e+00, -2.7002e+00, -2.1298e-01, -3.3738e+00,\n",
      "        -3.9626e+00, -8.7361e-01,  1.6330e+00, -4.0530e-01, -2.5986e+00,\n",
      "         5.1931e+00, -4.9246e+00,  5.9283e+00,  7.0322e-01, -5.2715e+00,\n",
      "         1.7706e+00,  3.6093e-03,  2.9787e+00,  1.8023e+00,  4.5037e+00,\n",
      "         4.1491e+00, -5.0086e+00,  2.8304e+00, -6.7761e+00, -1.6131e+00,\n",
      "         3.1030e+00, -7.4108e-01, -6.6344e-02,  8.7294e+00, -2.8990e-01,\n",
      "        -2.4898e+00, -5.8864e+00,  7.0400e+00, -5.0946e+00, -4.6184e-01,\n",
      "         3.8533e-01, -2.4418e+00,  2.5270e+00, -2.3711e+00,  1.6995e+00,\n",
      "         2.9339e-02, -5.1209e+00, -2.8351e+00, -7.6300e+00,  3.7635e-01,\n",
      "        -1.2412e+00, -8.1708e-02, -1.6783e+00,  2.0306e+00,  3.8866e-01,\n",
      "         3.2949e+00, -2.0632e+00, -1.0842e+00,  3.8667e+00, -4.9233e+00,\n",
      "        -4.4719e+00,  1.4897e+00, -1.9925e+00, -5.0145e+00,  4.8688e+00,\n",
      "         3.4179e+00, -3.1225e+00, -3.3827e+00, -2.3503e+00, -5.6035e-01,\n",
      "        -1.7340e+00, -2.0889e+00,  3.9123e-01,  7.4105e+00,  4.3279e+00,\n",
      "         1.2880e+00, -3.0574e+00, -3.1282e-01,  4.0478e+00,  2.9353e-01,\n",
      "        -1.8650e+00, -3.2571e+00,  5.1843e+00, -5.0219e+00,  4.9008e-01,\n",
      "        -2.3813e+00,  3.1718e+00, -4.1094e+00, -7.7047e+00,  1.3544e+00,\n",
      "         4.1686e+00, -2.2344e+00,  9.9851e+00,  7.2583e+00,  2.7500e+00,\n",
      "         3.1016e+00, -3.8355e+00, -3.2386e+00,  6.7120e+00,  4.1001e+00,\n",
      "         3.3082e+00,  4.2092e+00, -1.3627e+00,  5.7223e+00,  2.7738e+00,\n",
      "        -5.7553e+00, -1.7233e+00,  2.3039e+00,  3.3323e-01, -4.0872e+00,\n",
      "         1.9451e+00,  5.4706e+00, -1.2814e-01,  1.6041e+00, -2.7116e+00,\n",
      "         6.4094e+00,  2.0969e+00, -2.5715e+00, -1.0724e+00,  8.4452e+00,\n",
      "        -2.9047e+00, -7.1559e-01,  1.0002e+00,  3.4062e+00, -2.0334e-01,\n",
      "         2.2732e-01, -3.1752e+00, -3.3634e+00, -6.5274e-01,  9.6629e+00,\n",
      "         3.1738e+00, -9.7245e+00, -3.0113e-01,  2.8123e+00,  4.0509e+00,\n",
      "         5.1608e-01, -2.7976e+00, -4.0176e+00,  6.1981e-01, -4.4608e+00,\n",
      "         8.0998e+00, -4.2729e+00,  1.4686e+00,  2.7227e+00,  1.2631e+00,\n",
      "        -2.1092e+00,  1.7140e+00, -6.7823e+00,  6.8980e+00, -1.6280e+00,\n",
      "        -3.8437e+00, -5.4106e+00,  6.2556e+00,  2.6934e+00,  1.5199e+00,\n",
      "        -7.3314e+00,  3.1548e+00, -3.9446e+00, -1.1740e+00, -5.1045e+00,\n",
      "         1.3646e+00,  6.1397e+00,  4.3618e+00, -2.5729e-01, -3.6897e-01,\n",
      "         4.7251e+00,  4.7634e-01,  1.8215e+00, -2.1516e+00,  3.6727e+00,\n",
      "        -1.1475e+00, -2.6503e+00,  3.1847e+00,  4.2755e+00, -3.8686e+00,\n",
      "        -1.8565e+00,  2.7543e+00, -3.5417e+00,  5.7170e-01,  5.8162e+00,\n",
      "        -6.0792e-01, -5.5652e+00,  5.7914e+00,  1.4198e+00, -3.2205e+00,\n",
      "         2.2989e+00, -1.4120e-01, -3.6875e+00,  1.5675e+00, -2.1897e+00,\n",
      "         2.8670e+00, -1.8886e+00,  3.5918e+00, -1.1870e+00,  2.6905e+00,\n",
      "        -6.3051e-01,  4.2918e+00,  7.5812e-01,  2.4772e-01,  5.1785e-01,\n",
      "        -3.9748e+00,  5.3593e+00, -3.1866e+00,  5.0718e+00,  2.5495e+00,\n",
      "        -6.3212e-01, -3.4029e+00, -2.1900e+00,  2.1863e+00, -3.7755e-02,\n",
      "        -4.0826e+00,  2.6238e+00,  9.7899e+00,  5.8293e+00,  3.6448e+00,\n",
      "        -7.2764e+00,  1.0600e-01,  5.1786e+00,  1.0723e+00, -4.0616e+00,\n",
      "        -2.1304e+00,  5.7387e-01,  1.9304e+00,  4.9649e+00, -3.3315e-01,\n",
      "        -5.1604e+00,  1.8702e+00, -2.9451e+00,  3.4696e+00,  1.4624e+00,\n",
      "        -3.4567e+00, -2.1601e+00, -1.7772e+00, -6.7261e-01, -2.2673e+00,\n",
      "        -2.6888e+00, -1.7351e+00,  7.6955e+00, -2.7260e+00, -6.0990e+00,\n",
      "         4.6630e+00, -1.4322e+00,  1.9175e+00,  4.6483e+00,  7.2268e+00,\n",
      "         9.9589e-02, -2.6851e-01, -1.3116e+00, -3.6814e+00,  2.3830e+00,\n",
      "         3.8512e+00,  5.8422e+00,  4.1782e+00,  1.1719e-01,  3.0361e+00,\n",
      "        -1.2094e+00,  1.9002e+00, -2.0764e+00, -5.1201e-01,  4.6383e+00,\n",
      "        -1.4158e+00, -2.9361e-01,  6.7242e+00, -3.2070e+00, -1.5660e-01,\n",
      "         4.8738e+00,  3.4067e-01, -3.2068e+00,  8.1257e+00, -9.6805e-01,\n",
      "        -1.0940e+00,  7.5121e-01,  3.6947e+00,  3.2972e+00, -9.5196e-02,\n",
      "        -4.5848e-01, -8.7205e-01, -1.5549e+00, -2.6458e+00, -5.6087e+00,\n",
      "         1.6034e+00, -3.0322e+00,  3.3068e+00,  2.4960e+00,  3.7546e+00,\n",
      "         4.1376e+00, -3.0881e+00,  5.5949e-01,  8.5754e-01, -2.3999e+00,\n",
      "         8.2155e+00, -1.3638e+00,  1.3560e+00, -6.2807e+00, -1.1528e+00,\n",
      "        -6.3262e-01, -6.7930e+00,  3.8524e-01, -3.9740e+00,  2.7598e+00,\n",
      "        -6.0180e+00,  1.2198e+00,  1.6274e+00, -2.5818e-01,  2.2286e+00,\n",
      "        -3.9505e-01,  1.5963e+00, -3.2881e+00,  1.4620e+00, -2.5494e+00,\n",
      "        -1.0654e+01,  2.2111e+00,  3.4549e-02,  1.3166e+00, -1.8049e+00,\n",
      "        -9.3944e-01,  1.6400e+00, -8.8519e-01,  4.2903e+00, -1.6351e+00,\n",
      "         4.6491e+00,  3.8201e+00, -2.2017e+00,  1.7322e+00, -4.2772e+00,\n",
      "         3.1498e+00,  2.0918e+00,  1.0259e+01,  1.4810e+00,  1.3069e+00,\n",
      "         2.9908e+00,  5.6013e-01, -1.8177e+00,  2.1700e+00, -4.4305e+00,\n",
      "         4.9166e+00, -1.3779e+00, -4.7589e+00, -1.7665e+00, -5.0794e+00,\n",
      "        -7.5025e+00,  1.1686e+00,  5.3294e+00,  7.5543e+00,  1.5560e+00,\n",
      "        -3.8046e+00, -1.2945e+00,  1.8654e+00, -1.4736e+00, -2.6009e-02,\n",
      "        -2.0182e+00, -7.6507e+00, -4.8715e+00,  3.7355e+00, -2.4763e+00,\n",
      "         1.2203e+00, -5.3528e+00, -1.1344e+00, -4.8200e-01,  1.5985e-01,\n",
      "        -8.5924e-01,  2.6306e+00, -2.0628e+00, -1.2516e+00, -4.4414e+00,\n",
      "         9.3976e-01,  1.6312e-01, -2.3225e+00,  5.3463e+00,  1.0890e+00,\n",
      "         1.8292e+00,  4.0095e+00,  4.6297e+00, -2.7864e+00, -8.9147e-01,\n",
      "        -1.4158e+00, -1.5528e+00]), 'last_per_encoder_features': tensor([ 3.2338e+00,  2.3267e-01,  1.9298e+00,  3.1293e+00, -3.8693e-01,\n",
      "         1.1911e-01, -2.3682e+00, -3.6019e-01,  1.2491e-01, -1.0656e+00,\n",
      "         5.2936e+00,  2.7773e+00,  1.5649e+00, -6.4209e-01, -4.4660e-01,\n",
      "        -8.7474e-01, -7.2679e-01, -3.9458e+00, -1.1891e+00,  5.5661e+00,\n",
      "        -2.5080e+00,  1.3170e+00,  6.0586e-02, -2.5536e+00, -2.8127e+00,\n",
      "         1.4003e+00,  3.3015e+00, -1.3082e+00,  3.7491e-01,  3.7525e+00,\n",
      "         1.7888e+00,  8.5934e-01, -1.4472e+00, -2.7822e+00, -1.5573e+00,\n",
      "        -1.9809e+00,  6.4740e+00,  1.2897e+00, -4.5894e-01, -2.6831e+00,\n",
      "        -8.9856e-01,  2.1393e+00, -2.4102e+00, -6.9408e-01,  1.6904e+00,\n",
      "        -2.3151e+00, -2.8924e+00,  3.8132e+00, -2.4694e+00,  2.9607e+00,\n",
      "         1.1377e+00, -3.1545e+00, -3.1086e+00, -4.3732e+00,  3.7491e+00,\n",
      "        -7.4108e-01, -3.0854e+00, -4.4863e-01, -3.3211e+00,  1.3572e+00,\n",
      "        -4.3132e+00, -7.8063e-01, -1.8812e+00,  6.7955e-01, -3.0004e+00,\n",
      "         1.0218e-01,  3.6241e+00, -3.8128e+00, -2.3666e+00, -1.5517e+00,\n",
      "        -2.6904e+00,  1.7657e+00,  6.7718e-01,  2.3284e+00,  5.3629e-01,\n",
      "         3.7308e+00,  4.6973e-01,  1.1767e+00,  2.0338e+00, -2.2524e-01,\n",
      "         1.5862e+00, -1.6804e+00,  6.6731e-02,  1.3643e+00, -2.8057e+00,\n",
      "         2.0466e+00,  9.6028e-01, -1.6674e-01,  2.3412e+00, -4.5179e+00,\n",
      "         3.3901e+00,  1.6813e+00, -4.1837e-01,  4.0442e+00, -4.5519e+00,\n",
      "         4.1974e-01,  1.6527e+00,  1.3872e+00,  6.2810e-01, -4.0023e+00,\n",
      "        -8.0105e-01, -1.8720e+00,  3.4250e+00,  7.6286e-03, -3.6836e+00,\n",
      "         1.1063e+00, -3.1521e+00,  3.9879e-01,  1.7497e+00,  3.8681e+00,\n",
      "        -1.7927e+00,  9.6971e-01,  1.7850e+00,  1.0811e+00, -6.5036e-01,\n",
      "         5.2729e-01,  1.1369e+00, -4.5676e+00,  3.3883e+00,  1.3878e+00,\n",
      "        -3.2486e+00, -9.3917e-01, -3.4148e+00, -1.4130e+00, -1.6031e-01,\n",
      "        -2.0971e+00,  1.8944e+00,  5.3803e-02,  2.2753e+00, -1.1502e-01,\n",
      "        -3.7721e+00, -1.6567e+00, -2.4559e+00,  4.1099e+00,  1.5047e+00,\n",
      "         6.0008e-01,  2.7269e+00,  3.8545e+00, -7.4273e-02,  7.6416e-01,\n",
      "         4.6364e+00, -5.6938e-01, -4.9919e+00,  1.4771e+00, -1.9482e+00,\n",
      "         4.3250e+00, -2.5153e+00,  5.1857e+00,  3.3751e+00,  3.9504e+00,\n",
      "         2.2008e+00, -1.1230e-01,  3.0734e+00, -3.6183e+00, -5.8605e-01,\n",
      "        -3.3845e+00, -2.3121e+00,  1.2278e+00, -1.3076e+00,  1.4747e+00,\n",
      "         2.6751e+00, -5.1467e+00,  7.5568e-02,  5.3671e+00,  1.4516e+00,\n",
      "         3.7613e+00, -7.8310e-01,  1.4013e+00,  1.9185e+00,  5.1816e+00,\n",
      "         7.0182e-01,  1.6557e+00, -4.3582e+00, -3.8629e+00,  4.2693e+00,\n",
      "        -3.3388e+00, -5.9582e-01,  4.7503e+00,  8.2866e-01, -2.4502e+00,\n",
      "         1.9049e+00,  3.0588e+00,  6.9975e-01, -2.9076e+00,  9.0025e-01,\n",
      "        -3.4391e+00, -6.2399e+00, -3.2116e+00, -1.7095e+00,  1.3730e-01,\n",
      "        -6.3791e-01,  1.2658e+00, -2.1533e-01, -2.1612e+00, -2.6038e+00,\n",
      "        -7.2521e-02,  2.1140e+00, -3.7742e+00,  3.0219e+00, -3.9221e+00,\n",
      "         1.3807e+00,  6.6327e+00, -5.7396e+00, -7.2849e+00,  4.2359e-01,\n",
      "        -5.2356e+00,  8.9175e-02,  7.4388e-01,  1.0587e+00,  2.7084e-01,\n",
      "        -6.8275e+00, -1.5331e+00, -2.8741e+00,  5.0573e+00, -4.7391e-02,\n",
      "         7.6390e-01, -2.1694e+00, -2.1048e-02,  4.0565e+00, -2.6300e-01,\n",
      "        -2.9214e+00,  2.2646e+00,  1.3314e-01,  1.2463e+00,  7.4784e-01,\n",
      "        -3.2957e+00, -1.3732e+00, -2.3054e+00, -4.0863e+00, -3.9297e+00,\n",
      "        -1.8179e+00, -3.9794e+00, -4.0470e+00,  1.6607e+00, -2.1171e+00,\n",
      "         1.1920e+00,  1.5657e+00, -2.1325e+00,  1.1168e+00, -5.5560e+00,\n",
      "         1.2529e+00,  1.7944e+00, -4.9721e+00, -9.3786e-01,  8.9175e-01,\n",
      "         1.3876e+00, -2.2516e+00,  4.9051e+00,  9.7302e-01, -2.2149e+00,\n",
      "         1.9213e+00,  1.8117e+00,  2.0231e+00,  2.5926e+00, -4.2044e+00,\n",
      "         1.1225e+00,  2.8246e+00,  3.6709e+00, -2.6303e+00,  9.5788e-01,\n",
      "        -5.6983e+00,  3.6058e+00, -3.4928e+00,  2.1949e+00, -3.0186e+00,\n",
      "         1.5239e+00,  9.7444e-01, -1.8070e+00, -2.7498e+00,  2.9544e+00,\n",
      "        -1.1658e+00, -1.4650e+00,  1.8361e-01,  1.7581e+00, -5.9456e+00,\n",
      "        -2.0225e+00, -2.1435e+00,  2.3218e+00,  2.7579e+00,  2.1897e+00,\n",
      "        -4.9537e+00,  3.3348e+00, -1.8958e+00, -4.9453e-01, -2.2985e-01,\n",
      "        -2.7471e+00,  2.7585e+00, -3.1399e+00, -1.2538e+00, -1.8937e+00,\n",
      "         3.5548e+00, -8.5176e-02,  3.0890e+00, -4.9812e+00,  3.5201e-01,\n",
      "        -3.6417e-01,  1.9485e+00,  1.0121e+00, -1.1241e+00,  2.8581e+00,\n",
      "        -1.9528e+00, -6.1504e-01,  4.6464e+00, -4.6865e+00, -6.6013e-01,\n",
      "         5.9946e+00,  1.2056e+00,  1.8354e-01, -3.7932e+00,  4.7302e+00,\n",
      "         1.2248e-01,  1.3842e-01, -1.3366e+00,  4.1323e-01, -5.8237e-01,\n",
      "         2.9351e+00, -3.7018e+00,  2.6084e-01,  1.0671e+00,  2.3701e+00,\n",
      "        -3.6357e-01,  4.0226e+00,  1.2673e+00, -3.7144e+00,  3.6318e+00,\n",
      "         6.4479e+00, -9.1351e-01, -2.2054e+00,  2.2780e+00, -7.8682e+00,\n",
      "        -3.5341e+00, -1.1439e+00, -1.3584e+00,  1.1768e+00,  6.3366e-01,\n",
      "        -1.2893e+00,  2.3888e-02, -1.5118e+00, -1.5829e+00,  5.6466e+00,\n",
      "        -2.6070e+00,  3.2595e-01,  2.7478e+00, -2.1465e+00,  1.1823e+01,\n",
      "        -1.9064e+00,  3.4033e+00, -5.9479e-01,  6.2098e-02, -8.2261e-01,\n",
      "        -1.3202e+00, -4.4438e+00,  2.9334e+00, -5.3838e-01,  7.0116e-01,\n",
      "        -2.6877e+00, -1.5389e+00,  1.4680e-01,  1.7785e+00, -3.1061e+00,\n",
      "        -2.3576e+00,  1.0294e+00, -5.2095e+00, -3.0123e+00,  1.8601e+00,\n",
      "         7.9302e-01, -1.0311e+00,  2.8714e+00, -1.6268e+00,  4.8312e-01,\n",
      "        -3.8697e+00, -5.9141e+00,  4.5940e+00, -9.7427e-01, -1.4188e+00,\n",
      "         3.1737e-01,  3.1455e+00, -4.5668e-02, -1.8410e+00,  4.4376e+00,\n",
      "        -1.0471e+00, -1.1363e-01,  9.2275e-01, -1.2525e+00, -2.2935e-01,\n",
      "         3.8577e+00, -1.4300e+00, -1.2229e+00, -5.1959e+00, -2.8758e-01,\n",
      "         2.8194e-01, -3.0743e+00, -3.9975e+00, -1.0507e+00,  1.6811e+00,\n",
      "         1.9032e+00, -8.7577e-01,  1.3185e-01,  2.0499e+00,  1.9730e+00,\n",
      "        -1.4967e+00,  1.8372e-01, -3.1571e+00,  2.2138e+00, -2.8808e+00,\n",
      "         2.3193e+00,  3.3449e+00,  1.5074e+00,  5.0530e-01, -4.3380e+00,\n",
      "         3.4059e+00, -1.3119e-01,  5.4440e-02,  2.3663e+00,  3.0585e+00,\n",
      "         2.6161e+00,  3.9420e-01,  1.6687e+00,  2.6000e+00,  3.1212e+00,\n",
      "         2.5500e+00,  2.0428e+00,  1.1049e+00, -4.3755e+00,  2.6420e+00,\n",
      "        -2.0083e+00,  3.5497e+00,  1.3036e+00,  2.5543e-01, -3.7884e+00,\n",
      "        -2.2399e+00, -1.7780e+00, -2.9948e+00, -1.4503e+00,  2.6964e+00,\n",
      "         1.3744e-01, -1.1253e+00,  1.4716e+00,  3.5283e+00, -2.2030e+00,\n",
      "        -4.8368e+00,  2.0534e+00,  2.5306e-01, -4.7744e+00,  5.0573e+00,\n",
      "         2.7411e-01,  1.3691e+00, -1.3658e+00,  2.0123e+00,  2.2884e+00,\n",
      "         8.4668e-01,  2.0074e+00,  4.1866e+00,  1.5134e+00,  1.2394e+00,\n",
      "        -2.3605e+00, -1.8505e+00,  2.2327e+00,  1.0555e+00,  6.6356e+00,\n",
      "        -8.4205e-01,  4.2231e-01, -2.7072e-02,  1.8639e+00,  3.6851e-01,\n",
      "        -2.1224e+00, -1.8655e+00, -4.5021e-01, -2.4667e+00,  3.7924e+00,\n",
      "         1.8029e+00,  7.3143e-01, -3.7846e+00,  1.3289e+00,  2.2093e+00,\n",
      "         4.8800e-01, -1.9338e+00,  2.3060e+00,  5.6123e+00,  1.2356e+00,\n",
      "         6.9255e+00, -4.2587e+00,  2.8779e+00,  9.5974e-01,  4.9084e+00,\n",
      "         6.4139e+00,  1.7226e+00,  1.9074e+00,  1.9187e+00, -1.2965e+00,\n",
      "         9.2510e-01, -2.3302e+00, -6.8837e-02, -3.9010e+00, -2.2931e+00,\n",
      "        -6.8565e-01, -3.5461e+00, -3.3087e+00, -2.4025e+00, -7.1152e-01,\n",
      "         2.7465e-01, -4.3777e+00,  1.8202e+00, -2.4710e+00,  1.8866e+00,\n",
      "        -3.9067e+00,  9.0395e-01, -1.1061e+00, -3.8994e+00,  4.9526e-01,\n",
      "         2.0814e+00, -2.7817e-01])}, 'scene': {'emotion_logits': tensor([-1.4333,  0.7245,  0.2676, -0.8584,  0.4059, -0.1515,  1.7094]), 'personality_scores': tensor([0.6100, 0.5503, 0.4974, 0.6232, 0.5360]), 'last_emo_encoder_features': tensor([ 2.3787e-01, -3.8834e+00, -7.1912e+00,  5.3321e+00,  6.3292e+00,\n",
      "        -1.5183e+00,  3.9924e+00, -3.5136e+00, -6.4992e+00,  3.3300e+00,\n",
      "         1.3883e+00,  1.5157e+00,  5.1890e-02,  3.9729e+00,  2.4476e+00,\n",
      "         4.8612e+00, -3.5312e+00,  2.5139e+00, -2.2752e+00, -5.1269e+00,\n",
      "        -1.7450e+00,  1.7746e+00, -5.2801e+00,  8.3871e+00,  1.3502e+00,\n",
      "         2.1368e-01,  6.9356e+00, -2.4151e+00,  5.6247e+00, -7.3593e+00,\n",
      "         3.0710e-01,  3.1934e+00, -2.5554e+00,  3.7122e+00, -6.0992e+00,\n",
      "        -7.3464e+00,  2.1698e+00,  1.3134e+00,  4.6045e+00,  1.0322e+01,\n",
      "        -1.9648e+00, -5.6326e+00, -2.3873e+00,  5.3190e+00, -5.2574e+00,\n",
      "         4.2398e+00,  3.9525e+00,  4.8422e+00, -7.8822e+00, -9.0932e-01,\n",
      "         7.8881e+00, -1.7802e+00, -1.2834e+01, -7.6085e+00,  2.1936e+00,\n",
      "        -3.3690e+00, -2.2804e+00, -3.7461e+00, -1.4956e+00,  5.5032e+00,\n",
      "        -2.8310e+00, -2.9911e+00,  5.4648e+00, -6.1285e+00, -9.3055e+00,\n",
      "        -1.0810e+00, -5.6580e+00, -3.0876e+00, -1.4306e+01,  3.3087e-01,\n",
      "         6.0525e+00,  8.4826e-01,  7.1211e-01,  9.8361e+00, -1.6598e+00,\n",
      "        -1.1091e+00,  3.2384e+00, -5.1005e+00, -1.4814e+00, -6.9780e-01,\n",
      "         8.3499e+00,  7.3279e+00,  5.1125e+00, -1.6738e+00,  9.9289e+00,\n",
      "        -3.4881e+00, -3.1973e+00, -3.2181e+00, -3.6356e+00, -5.5434e+00,\n",
      "         9.2431e-01, -1.8493e+00, -1.6358e-01, -3.1843e+00,  8.2259e-02,\n",
      "         3.9277e+00,  1.0521e+01, -4.7876e+00, -6.2171e+00,  4.7177e+00,\n",
      "        -4.1876e+00, -2.6379e+00, -6.1906e+00,  3.0403e+00,  5.3519e+00,\n",
      "         5.0244e+00,  4.2737e+00, -3.6309e+00,  1.8119e+00,  5.0457e+00,\n",
      "         4.7204e+00, -3.2511e+00, -2.4966e+00, -1.0271e+01,  7.3345e+00,\n",
      "        -8.1386e+00,  2.8831e+00,  2.3173e+00, -1.4867e+00, -7.8071e-01,\n",
      "         4.6816e+00, -8.2825e+00, -2.3328e+00, -1.3373e-01, -6.8470e+00,\n",
      "        -9.0404e+00, -8.0676e+00, -2.2382e+00,  6.6907e+00,  2.0147e-01,\n",
      "         5.4555e-01, -1.8269e+00,  8.8182e-01,  6.1917e+00, -5.3116e+00,\n",
      "        -6.1779e+00,  2.3655e+00,  2.5059e+00, -2.3998e+00, -4.4474e+00,\n",
      "         5.7433e+00, -1.0222e+01,  6.8318e+00, -4.5264e+00, -3.4410e-01,\n",
      "        -4.2875e+00,  4.0702e+00,  9.3599e-02,  1.8182e+00, -4.6854e+00,\n",
      "         2.0914e+00,  1.1476e+00,  3.5378e+00,  3.2371e-01,  3.8176e+00,\n",
      "        -9.9156e-01,  4.0436e-01, -6.5355e+00, -2.5413e+00, -2.0900e+00,\n",
      "         5.3924e+00,  1.1613e+00,  1.0549e+01, -3.0336e+00,  1.1147e+00,\n",
      "        -3.6649e+00, -4.5883e+00, -1.0492e+01,  8.9544e+00, -1.5124e+00,\n",
      "        -7.7820e-01, -1.3490e+00, -6.6801e+00,  3.7681e-01, -1.8330e+00,\n",
      "        -9.6318e+00,  4.2866e+00, -2.9616e-01,  5.5267e+00, -7.5601e-01,\n",
      "         4.5164e-01,  6.4039e+00, -7.6750e-01,  9.4643e+00, -8.9231e+00,\n",
      "        -2.3442e+00, -8.5921e+00, -1.1831e+01, -8.0583e+00, -6.1528e+00,\n",
      "        -1.8007e+00,  8.8117e-01,  1.0867e+00,  4.4776e+00,  5.3892e+00,\n",
      "        -4.8403e+00,  5.1348e+00,  6.9359e+00,  3.1614e+00,  1.3383e+00,\n",
      "        -3.6373e+00, -2.5110e+00,  4.5568e+00,  3.5242e+00,  1.1997e+01,\n",
      "        -6.4365e+00,  3.2448e+00, -4.8026e+00, -7.1843e+00, -8.5060e+00,\n",
      "         8.4152e+00,  5.9144e+00, -4.1653e+00, -6.2699e+00, -5.2620e+00,\n",
      "        -2.3535e+00, -3.3096e+00, -1.1033e+00, -2.8798e+00,  8.8135e+00,\n",
      "        -3.3020e+00,  1.0644e+00,  6.5499e-01,  3.8572e+00,  8.0724e-01,\n",
      "         2.4111e+00,  7.4230e+00, -7.4700e+00, -2.2926e+00, -2.7499e+00,\n",
      "        -1.4253e+00,  1.7071e+00,  9.2847e-01,  4.0783e-01,  4.6657e+00,\n",
      "         1.1054e+00,  6.3835e+00,  8.0056e+00,  3.4464e+00,  3.7628e+00,\n",
      "        -2.0915e+00,  4.3032e+00, -3.2040e+00,  2.8909e+00, -5.4711e+00,\n",
      "        -2.7930e+00, -4.7009e+00,  6.1352e+00,  2.3082e-01, -5.4912e+00,\n",
      "         6.8037e+00, -2.5187e+00, -3.3790e+00, -8.6859e+00,  3.7653e+00,\n",
      "         1.0826e+01, -2.0312e-01,  3.4327e+00, -2.2023e+00,  6.1486e+00,\n",
      "        -1.9101e+00,  5.9995e-01, -6.2679e-01, -4.5138e-01,  3.3159e+00,\n",
      "        -2.4228e+00,  2.3850e+00, -1.4199e+01,  4.0967e+00, -3.3672e+00,\n",
      "        -1.6761e+01, -7.6468e+00, -3.4368e-02,  8.2079e+00, -1.4754e-01,\n",
      "        -7.8879e+00, -1.3252e-01, -1.5666e-01, -2.7280e+00,  1.1280e+01,\n",
      "        -6.5206e+00, -7.6729e+00,  3.8400e+00, -1.6883e+00,  4.6609e+00,\n",
      "        -8.4115e-02,  1.3188e+01, -6.1914e+00, -1.9045e+00,  2.1923e+00,\n",
      "        -4.1606e+00, -6.7669e+00, -9.6703e+00,  1.0640e+01,  7.1485e+00,\n",
      "         3.1097e+00, -8.7797e-01, -7.8766e+00,  4.2617e+00,  3.1649e+00,\n",
      "        -1.3917e+01, -3.7073e+00, -7.3794e+00,  3.1572e+00,  2.6021e-01,\n",
      "        -5.9029e+00,  9.9146e+00,  1.0009e+01,  1.1725e+01, -5.8241e-01,\n",
      "        -2.1396e+00,  6.7563e+00, -1.0472e+01,  8.8183e+00,  5.2901e+00,\n",
      "         1.8175e+00,  6.9565e-01, -7.4367e-01, -9.9724e-01,  9.7592e+00,\n",
      "         5.1865e+00, -6.9197e+00, -4.7459e+00, -8.6184e-02, -2.4251e+00,\n",
      "        -2.2968e+00, -4.0255e+00,  8.2761e+00, -6.1851e+00, -1.3453e+00,\n",
      "         3.3700e+00,  1.0880e+01, -9.8208e+00, -3.2348e+00,  1.8334e+00,\n",
      "         9.7019e+00,  5.9807e+00,  3.4006e+00, -2.3107e+00,  7.8787e+00,\n",
      "        -2.2966e+00, -1.2777e+01, -8.5967e-01, -6.4830e+00,  8.1516e-01,\n",
      "         5.3417e+00,  9.3494e+00,  1.2564e+00,  1.2110e+00,  2.7222e-01,\n",
      "         5.8424e+00,  4.6230e+00, -2.5580e+00,  2.6289e+00,  6.5890e+00,\n",
      "        -1.1180e+00,  9.9163e+00,  4.5176e+00, -2.8784e+00, -3.0252e+00,\n",
      "        -2.1177e+00, -8.8051e+00,  2.7545e+00,  1.0904e+00, -1.1878e+01,\n",
      "        -1.3318e+00, -5.7813e+00,  1.5563e+00, -1.0464e+00,  1.0525e+00,\n",
      "        -1.7684e-01,  1.7452e+00,  1.9554e+00,  4.2497e+00,  5.8221e+00,\n",
      "         4.2908e+00,  2.4817e+00,  2.0982e+00, -8.4228e-01, -4.2798e+00,\n",
      "         7.0708e+00, -4.3140e+00, -2.7967e+00, -5.9767e+00, -1.1393e+00,\n",
      "        -7.3027e+00, -6.9269e-01, -1.2012e+01, -3.7576e+00, -2.9896e+00,\n",
      "        -9.4580e+00,  1.0103e+00,  1.0660e+01,  4.1728e-01,  1.7727e+00,\n",
      "        -6.1872e+00, -3.7642e+00, -2.4434e+00, -7.8747e+00, -9.5886e+00,\n",
      "        -3.8489e+00, -1.4877e+00, -1.1085e+00, -5.3650e+00, -2.7637e+00,\n",
      "        -9.1552e+00, -7.1608e+00,  2.3885e+00, -4.0454e+00, -1.2996e-02,\n",
      "         7.0234e-01,  1.8636e+00, -6.2101e+00, -1.1594e+01, -1.7261e+00,\n",
      "        -6.2137e+00,  6.1855e-02,  1.7723e+00, -5.4557e+00, -7.8594e+00,\n",
      "        -1.4333e+00,  7.6275e+00,  2.0836e+00, -9.0407e+00,  5.2556e+00,\n",
      "        -4.1504e+00, -5.1819e+00, -2.8227e+00, -8.6919e+00,  5.6820e+00,\n",
      "         8.4793e+00,  2.8134e+00,  2.5828e+00,  4.4889e+00,  1.0020e+00,\n",
      "         4.2878e+00,  3.1450e-01,  8.0892e-01,  1.5304e+00, -1.0117e+00,\n",
      "        -6.6626e+00, -5.9094e+00, -9.6859e+00,  9.6772e-02,  1.0876e-01,\n",
      "         2.4252e+00, -4.6321e+00,  2.8069e+00, -2.0257e+00,  1.0270e+01,\n",
      "        -1.0158e+01, -3.9882e-01, -1.1430e+00,  3.3333e-01,  3.5036e+00,\n",
      "         2.5826e+00,  8.3437e+00, -4.3181e+00, -3.3626e+00,  1.1668e+01,\n",
      "        -3.2377e+00,  4.7443e+00, -7.3465e+00,  5.7454e+00, -8.9468e+00,\n",
      "        -4.0794e+00, -6.6916e+00,  6.6732e+00,  1.5221e+00,  4.2347e+00,\n",
      "        -6.9635e+00,  3.3945e+00,  7.3129e+00, -1.3718e+00,  5.0162e+00,\n",
      "         9.2304e+00,  3.7794e+00, -1.1852e+01, -1.0663e+01,  6.0260e+00,\n",
      "         5.6517e+00,  1.1836e+00,  2.8217e+00,  6.1433e+00,  1.0218e+00,\n",
      "        -5.6199e-01,  8.1502e+00, -6.3567e-01,  6.4856e+00,  1.9782e+00,\n",
      "         2.0726e+00,  2.5245e-01,  2.1677e+00,  4.4590e+00,  5.0634e+00,\n",
      "         6.3268e+00, -1.0423e+01,  5.0981e+00, -3.7986e-01,  6.6475e+00,\n",
      "         1.0178e+00,  9.8995e-01,  3.3177e+00,  4.0010e+00, -1.2676e+01,\n",
      "         1.6655e+01, -2.0564e+00, -6.3273e+00,  2.2476e+00, -5.1166e+00,\n",
      "        -1.6911e+00, -3.6893e+00, -2.0025e+00,  2.7875e+00,  9.5758e+00,\n",
      "         2.6204e+00,  8.6320e+00, -8.3483e-01, -6.8274e+00,  6.8998e+00,\n",
      "        -7.9835e+00,  9.5381e-01, -1.2970e+00,  7.3481e+00, -4.5430e+00,\n",
      "         2.1745e+00,  5.8327e+00,  3.0674e+00,  3.7625e+00,  4.7538e+00,\n",
      "         2.6105e+00,  5.8152e+00, -4.9078e+00,  2.5384e+00,  1.3249e+00,\n",
      "        -1.5365e+00,  6.1200e+00,  3.6077e+00, -1.4039e+01,  1.3885e+01,\n",
      "        -1.7664e-02, -3.4103e+00, -7.2147e+00,  1.0290e+01, -7.6213e+00,\n",
      "        -2.6288e+00,  1.4286e+00, -8.7450e+00, -6.3688e+00,  2.4375e+00,\n",
      "         7.2051e+00, -6.2090e-01, -2.0419e+00,  4.3989e+00, -9.1522e-01,\n",
      "         2.9667e+00,  2.4220e+00,  8.9887e+00,  4.2168e+00,  1.7405e+00,\n",
      "        -9.4571e+00,  5.4345e+00, -2.4910e+00,  7.8425e-01, -5.8418e+00,\n",
      "         5.5463e-01,  5.1685e+00, -1.4832e+00,  2.5930e+00,  1.6321e+00,\n",
      "         1.4644e+00,  2.9812e+00, -1.3767e+00, -2.5237e-01, -8.2222e+00,\n",
      "         6.3669e-01,  2.6061e+00,  7.3695e+00, -1.1048e+01, -4.6053e+00,\n",
      "        -4.9406e-01,  5.3344e-01, -4.4758e+00,  5.3481e+00, -1.0148e+00,\n",
      "         1.0995e+01,  3.2176e+00,  5.3637e+00,  1.8544e+00, -8.7966e+00,\n",
      "         5.7315e-01, -6.9909e-01, -7.8226e+00, -1.5315e+00, -5.3935e+00,\n",
      "         1.8618e+00, -3.5726e+00,  6.3574e+00, -4.0371e+00, -4.3491e+00,\n",
      "         3.9022e+00, -5.6799e+00,  2.2404e+00,  1.1779e+00,  2.4978e+00,\n",
      "         8.8711e+00, -1.2098e+00, -4.0397e-01, -2.7559e+00, -4.2358e+00,\n",
      "         1.6956e+00, -4.0048e+00, -7.5931e-02,  3.8934e+00, -1.6308e+00,\n",
      "        -3.3488e+00, -6.1408e+00,  3.0229e+00,  8.5581e+00, -6.0708e+00,\n",
      "         1.2970e+01,  1.0041e+00,  5.5108e+00, -4.2719e+00, -8.6883e+00,\n",
      "         4.7743e+00, -9.9884e+00, -1.6041e+00,  1.4886e+00, -2.3462e+00,\n",
      "        -1.8657e+00, -2.4336e+00,  1.4318e+00, -8.1583e-01, -1.7606e+00,\n",
      "        -2.8726e+00, -1.0231e+00, -2.4061e+00, -3.6037e+00,  5.9612e+00,\n",
      "        -3.9139e+00, -1.0272e+01,  2.6960e+00,  1.2882e+01,  4.0967e+00,\n",
      "        -5.6563e+00,  2.2516e+00,  4.0383e+00,  2.6991e+00,  8.7077e+00,\n",
      "        -5.5376e+00,  7.8236e+00,  8.7870e+00,  1.2516e+01,  3.7844e+00,\n",
      "        -4.7775e-01,  6.1582e+00,  5.5258e+00, -1.9868e+00, -6.0468e+00,\n",
      "         3.9003e+00,  1.2224e+01,  2.1782e+00,  5.8556e+00, -6.1916e+00,\n",
      "         8.1870e-01, -3.6082e+00, -3.6370e+00,  1.2586e+01, -3.1409e+00,\n",
      "         3.5918e+00,  4.0803e+00, -9.6079e+00,  4.5173e-01,  9.5448e+00,\n",
      "         2.5711e+00, -1.1686e+00, -4.8172e-02, -3.1872e-01, -1.0429e+00,\n",
      "         8.8244e+00, -6.0428e+00,  6.3701e+00,  1.0094e+00,  1.0790e+00,\n",
      "         3.9488e+00, -3.7748e+00,  4.4448e+00, -9.3186e+00,  2.1814e+00,\n",
      "        -2.9024e+00,  6.3203e-02,  5.4036e+00,  4.3538e+00,  1.6868e+00,\n",
      "        -1.4195e+00, -8.7651e+00,  5.0847e+00, -5.7335e+00, -1.8023e+00,\n",
      "        -9.0781e+00, -2.6553e-01, -3.3247e+00,  1.7621e+00, -9.7558e+00,\n",
      "         6.2613e+00,  4.8260e+00,  4.1239e+00, -5.3657e+00,  2.6537e+00,\n",
      "        -3.8792e+00,  3.4451e+00, -3.9507e+00, -1.9084e+00, -8.3392e+00,\n",
      "        -8.3460e+00,  3.2348e+00,  8.4944e-01,  8.4488e+00, -6.9818e-03,\n",
      "        -9.1890e+00, -1.1923e+00,  1.3151e+00, -5.7928e+00, -2.0487e+00,\n",
      "         8.4857e+00, -4.3166e+00, -9.7413e-01, -1.1084e+00,  5.7486e-02,\n",
      "         2.8036e-01, -2.6529e+00, -3.1201e+00, -1.9452e+00, -2.0102e+00,\n",
      "        -9.6492e-01, -6.0166e-01,  1.0709e+01, -9.5378e+00, -1.8367e+00,\n",
      "        -5.8738e+00,  9.3012e+00,  1.3789e+01, -2.3880e+00,  4.3237e+00,\n",
      "         4.5467e+00,  6.3141e+00,  1.6710e+00,  6.3429e+00, -3.4105e+00,\n",
      "         6.0107e+00,  1.3215e+01,  3.1614e+00,  2.9405e-01,  1.4334e+00,\n",
      "        -2.4327e+00,  3.9065e+00,  3.1479e+00,  9.4400e+00, -4.8290e+00,\n",
      "        -6.9001e+00,  3.8859e+00, -9.5621e-01, -2.0298e-01, -2.0547e+00,\n",
      "        -6.5331e+00,  9.2660e-01, -1.4366e+01]), 'last_per_encoder_features': tensor([-1.0079e+01,  2.9155e+00, -4.1954e+00, -2.4917e+00,  3.6412e+00,\n",
      "        -1.7523e+00, -5.5399e+00, -9.1149e-01, -1.7929e+00, -2.1998e+00,\n",
      "        -3.4728e+00, -2.5975e+00, -2.8647e+00, -7.4567e+00,  2.5412e-01,\n",
      "        -2.2627e+00, -4.4851e+00,  1.2590e+00,  1.1041e-01, -2.2187e+00,\n",
      "         5.3155e+00, -3.8029e-02, -1.2715e+01,  5.3149e+00, -6.9754e-01,\n",
      "         6.0376e+00,  3.2892e+00,  3.5746e+00, -2.9349e+00,  5.1487e+00,\n",
      "        -2.4690e+00, -1.8876e+00,  2.4121e+00,  3.1189e+00, -2.7554e+00,\n",
      "         1.9466e+00, -2.1079e+00,  2.0291e+00,  6.9235e+00,  5.2300e-01,\n",
      "        -7.9751e+00, -3.4787e+00, -3.5319e+00, -7.0688e+00,  4.2677e+00,\n",
      "         1.8289e+00,  3.9868e+00,  1.1189e+00, -6.8893e-01, -4.1910e-01,\n",
      "        -6.7827e+00,  1.7373e+00, -9.3519e+00, -6.9381e+00,  4.3950e+00,\n",
      "         1.9485e+00,  4.1381e+00, -7.1808e-02, -4.2620e+00,  6.0565e-01,\n",
      "         1.7443e+00, -1.2201e+01,  4.7155e+00,  2.9244e+00, -3.5750e+00,\n",
      "         3.3379e+00,  2.9972e+00,  4.4150e+00,  2.9294e+00,  6.0864e+00,\n",
      "        -4.7239e-01, -1.2821e+00, -4.1894e+00, -5.7459e-01,  5.6957e+00,\n",
      "         7.4972e+00, -5.7049e+00, -1.9128e+00, -1.1967e+01, -1.0975e+00,\n",
      "        -4.3681e+00, -2.6959e+00,  2.4519e-01, -3.3275e+00, -6.5593e-01,\n",
      "         4.5439e+00, -8.4673e+00,  4.2297e+00,  2.9913e+00, -9.8971e+00,\n",
      "        -1.9554e-02, -8.9192e+00, -5.6847e-01, -4.0654e+00, -2.6646e+00,\n",
      "        -5.4103e+00, -9.6924e+00, -4.4935e+00, -1.3544e+00,  3.4089e+00,\n",
      "         4.2311e+00, -4.2349e+00,  6.2168e+00, -4.3542e+00,  1.1235e+00,\n",
      "         3.9927e+00,  6.3132e+00, -9.7299e-01, -4.9124e+00,  1.7469e+00,\n",
      "         6.6689e+00,  1.3913e+00, -4.3118e+00,  5.2006e-01,  6.4041e+00,\n",
      "        -8.0058e+00,  1.0746e+00, -1.6602e+00,  7.7346e+00,  2.0305e+00,\n",
      "         4.7837e+00, -9.1402e-01, -2.1465e+00,  8.2245e-01, -3.5189e+00,\n",
      "        -2.3320e+00,  7.5443e+00, -2.0450e+00, -1.8766e+00,  3.0040e+00,\n",
      "         5.7866e-01, -4.9976e+00, -2.7760e-01, -4.9023e+00, -6.3192e+00,\n",
      "        -1.3560e+00, -9.9203e+00, -1.1500e+01, -7.4354e-03,  3.8488e+00,\n",
      "         1.4185e+00, -3.6404e+00,  1.3103e+01, -1.2346e-01, -1.1764e+01,\n",
      "        -5.3660e+00, -7.1108e+00,  9.9225e-01,  1.2177e+01, -7.3531e+00,\n",
      "         1.1753e+00, -5.0549e+00, -9.5461e-01,  3.2593e+00,  3.5030e+00,\n",
      "         1.0364e-01, -6.3504e+00, -7.4612e+00, -4.5907e+00,  1.6883e+00,\n",
      "         4.5138e+00, -2.9435e+00,  1.3296e+00,  5.3060e+00,  8.9975e-01,\n",
      "        -6.5242e+00, -2.5203e+00,  2.0432e+00,  5.8720e-02,  4.6742e+00,\n",
      "        -3.8783e+00, -1.9073e+00, -9.1184e-03,  3.5182e+00,  6.0409e-01,\n",
      "         1.2120e-01,  8.5030e+00, -4.3152e-01, -6.5115e+00, -7.5184e+00,\n",
      "         1.3175e+01, -9.8047e-01,  3.3232e+00,  6.8978e+00,  6.5943e-01,\n",
      "         7.1717e-01,  3.0076e+00, -6.6217e+00,  4.3328e+00,  9.8964e+00,\n",
      "         9.0186e+00,  4.4809e+00, -4.1182e+00, -5.0461e+00, -9.4469e-01,\n",
      "        -3.1796e+00, -3.1924e-01,  5.9478e+00, -1.1675e+00, -3.4984e+00,\n",
      "        -2.1468e+00,  1.4056e-01,  7.3061e+00,  4.5638e+00, -8.0359e-01,\n",
      "         1.1512e+00,  1.0624e+00, -3.8957e+00, -2.0937e-01, -7.7410e-01,\n",
      "         1.9600e+00,  5.0869e-01, -3.8912e+00, -5.3181e+00,  9.1983e+00,\n",
      "         1.1948e+00,  6.8131e+00, -4.0197e+00,  6.3676e+00,  6.6814e-01,\n",
      "        -2.2782e+00, -4.6407e+00,  7.9976e+00,  1.5078e-03,  6.2491e+00,\n",
      "         1.6277e+00, -4.2905e+00,  3.9895e+00,  1.1564e+00,  1.6725e+00,\n",
      "        -3.3988e-01,  2.3487e-01,  2.3137e+00,  3.2055e+00, -2.5614e+00,\n",
      "        -6.2257e+00, -3.4862e+00,  6.1842e-01, -2.5337e-01,  2.4179e+00,\n",
      "         1.0395e+01, -1.0511e+01, -1.7698e+00, -1.2742e+00,  3.3799e+00,\n",
      "         6.3335e+00, -2.0555e+00, -3.7696e+00, -6.2405e+00,  3.3536e+00,\n",
      "         6.0006e+00, -4.2116e-01, -1.4156e+00,  1.2132e+00, -7.4987e+00,\n",
      "         1.0727e+01,  3.9252e+00, -2.2865e-02,  3.4125e+00, -4.8078e+00,\n",
      "        -2.2488e-01,  6.7128e-01,  1.0756e+00, -2.2097e+00, -3.9168e+00,\n",
      "        -3.0231e+00,  5.9489e+00,  1.8459e+00,  6.8214e+00, -1.3110e+00,\n",
      "        -1.3717e+00,  2.8867e+00,  7.1667e+00, -2.9858e+00,  2.3666e+00,\n",
      "        -5.4298e+00,  1.4686e-01,  4.8979e+00, -4.6937e+00, -1.5482e+00,\n",
      "        -1.9569e+00,  3.9718e+00, -4.8561e+00, -4.8697e+00,  2.2008e+00,\n",
      "        -9.5520e+00,  8.9195e-01, -2.1908e+00,  1.1687e+01,  4.8186e+00,\n",
      "         5.1032e+00,  2.5232e+00,  2.2995e+00, -5.1799e+00, -4.5348e+00,\n",
      "         8.1746e+00,  1.8391e+00, -4.5937e+00, -5.8388e+00, -6.2387e+00,\n",
      "        -8.1238e+00, -8.5082e-01,  7.3760e+00,  5.7515e+00, -6.7138e+00,\n",
      "        -3.8682e+00, -2.3334e+00,  2.4963e+00,  2.9068e-01,  3.1797e-01,\n",
      "         3.6475e-01,  3.7657e+00, -1.1861e-01,  6.6105e+00,  3.9459e+00,\n",
      "        -1.6686e+00, -3.0625e+00, -2.4927e+00,  1.3261e+00,  6.9836e+00,\n",
      "         1.5561e+00, -7.8775e-01, -1.2599e-01,  3.5080e+00,  4.1035e+00,\n",
      "        -6.0749e+00,  2.8163e+00, -8.0009e-01, -1.5545e+00,  1.8709e+00,\n",
      "         2.2479e+00, -5.1186e+00, -4.8471e+00, -1.7140e+00, -4.7540e+00,\n",
      "         2.0400e+00,  2.1510e-01,  8.3550e-01,  7.2478e-01,  4.6791e-01,\n",
      "        -5.6867e+00, -6.7494e+00,  5.7889e+00,  6.0867e+00,  5.9043e+00,\n",
      "         7.4780e+00, -4.3748e-01,  3.6545e+00, -3.9576e+00, -5.5238e+00,\n",
      "         4.6001e+00, -7.9374e+00, -6.4066e-01,  1.5825e+00, -4.8892e+00,\n",
      "         5.4220e+00, -1.6381e+00, -6.0520e+00,  4.2351e+00,  1.5452e+00,\n",
      "        -1.2869e+00,  6.9509e-01,  4.8280e-01, -2.8673e+00, -1.7430e+00,\n",
      "        -5.3874e+00, -5.0257e-01, -3.5964e+00, -1.4988e+00, -9.3882e+00,\n",
      "        -2.1293e+00,  3.5110e+00,  1.7807e+00,  9.4149e-01, -2.2054e+00,\n",
      "        -2.9870e+00, -1.7639e+00, -2.8895e+00,  4.1761e+00,  4.4474e-01,\n",
      "         4.2251e+00, -7.4885e+00,  2.3097e+00, -1.9600e+00, -3.3451e+00,\n",
      "        -7.5039e-01,  4.0444e+00, -8.2884e+00,  1.2641e+00,  2.4458e+00,\n",
      "        -7.7911e-01, -1.1043e+01, -9.3978e+00, -4.4462e+00,  1.8801e+00,\n",
      "         7.2047e+00, -9.4767e+00,  8.6167e+00,  3.8799e+00, -7.3834e-01,\n",
      "        -2.2369e+00, -4.6751e-02,  6.5024e-01,  8.9079e-01, -4.0577e+00,\n",
      "         4.6902e+00,  5.1650e+00,  2.3280e-01,  6.4196e+00,  3.4891e+00,\n",
      "        -5.2933e+00,  3.7874e+00, -1.9033e-02,  1.9205e+00, -2.6620e+00,\n",
      "         4.5240e+00, -8.2425e+00,  3.8450e+00, -3.3491e+00, -4.1970e+00,\n",
      "         1.6880e+00,  3.6000e+00, -3.1377e+00,  1.5141e+00,  1.8200e+00,\n",
      "         5.8746e+00, -2.4835e+00,  1.8183e+00, -6.5150e+00, -9.7107e-01,\n",
      "        -3.2948e-01,  2.5415e-01,  5.2651e-01,  7.4216e+00, -9.0267e-01,\n",
      "        -4.1788e+00,  2.2370e+00,  9.6179e+00,  4.9099e-01, -5.3301e-01,\n",
      "         3.5424e+00, -4.4684e-01, -8.0730e-01, -9.2975e-01,  5.6016e+00,\n",
      "         1.8706e+00, -3.8189e+00,  4.1242e+00,  3.8238e+00,  3.7290e-01,\n",
      "         2.6519e+00, -1.2764e+00, -2.1649e+00,  3.5609e+00, -8.5442e-01,\n",
      "        -4.4107e-01,  9.3796e+00,  2.9520e+00, -3.8170e-01, -1.3617e+00,\n",
      "         5.4898e-01,  9.1378e+00,  4.5382e+00,  7.4073e+00,  2.1877e+00,\n",
      "         3.4941e+00, -1.0535e+01,  3.0390e+00, -3.0659e+00, -1.7488e+00,\n",
      "        -9.3210e+00,  2.5417e+00, -6.3350e+00,  3.4418e+00, -5.2338e+00,\n",
      "        -5.8885e-01,  1.6319e+00,  1.6491e+00,  1.7342e+00, -7.9318e-01,\n",
      "         6.8204e+00, -6.1409e+00,  3.5772e+00,  1.9219e+00,  5.5041e-01,\n",
      "        -3.3191e+00,  2.2557e+00, -8.9615e+00, -7.5672e+00, -1.3450e+00,\n",
      "         4.8523e+00, -6.8862e-01, -8.8473e+00, -3.1955e+00, -2.0104e+00,\n",
      "         9.6998e+00, -1.6352e+00, -4.4211e+00,  4.6420e+00,  2.3833e+00,\n",
      "         2.5475e+00, -4.0904e+00, -7.0739e+00,  1.7430e+00,  1.3902e+00,\n",
      "        -3.9033e+00, -2.1125e+00, -2.6082e+00, -5.2099e+00,  9.5807e+00,\n",
      "        -1.4032e+00,  3.6701e+00, -4.7797e+00,  6.9348e+00,  4.8764e+00,\n",
      "        -2.6500e-01,  4.9907e+00,  2.0800e-01, -7.3440e+00,  1.7602e+00,\n",
      "        -1.3062e+01,  1.0485e+01, -3.5736e+00,  1.4996e+00, -4.1021e+00,\n",
      "        -4.4909e-01,  6.9055e+00,  7.0265e+00, -5.9789e-01, -4.5568e+00,\n",
      "        -3.5238e+00, -8.0854e+00,  6.9935e-01,  5.3185e+00,  9.5748e-01,\n",
      "        -7.0918e+00,  4.2332e+00, -5.4036e+00, -5.0974e+00,  7.8156e-01,\n",
      "         3.9248e+00, -1.2459e-01,  4.5356e+00, -7.9295e-01,  1.5595e+00,\n",
      "         9.2243e+00, -2.0316e+00, -7.9548e-01,  2.8317e-01,  2.6292e+00,\n",
      "         6.0321e+00,  4.6681e+00,  1.6802e+00,  9.6592e+00, -5.4792e+00,\n",
      "         3.9242e+00, -1.2946e-01, -1.0277e+01, -2.2234e-01, -5.3529e+00,\n",
      "        -1.4128e+00,  2.2767e+00,  8.4789e+00,  3.6149e+00, -5.2660e-01,\n",
      "        -8.4713e+00,  9.4161e+00, -1.5926e+00,  6.4449e-01,  4.0824e+00,\n",
      "        -1.3423e+00, -2.9161e+00, -2.3765e+00,  1.0254e+00, -1.9889e+00,\n",
      "         5.9667e+00,  7.9947e+00, -7.4879e+00,  7.8507e+00, -4.2025e+00,\n",
      "        -7.7323e-01, -2.4850e-01,  2.0151e-01,  7.2853e+00, -3.9316e+00,\n",
      "        -7.0431e+00,  3.1569e+00, -1.4668e+00, -1.1118e+00,  3.7967e-01,\n",
      "        -4.9894e+00, -1.4859e+00,  2.2884e+00,  2.4502e+00, -8.3760e+00,\n",
      "         9.7899e+00, -1.1879e+00, -1.8145e+00, -2.0571e+00, -1.1680e+00,\n",
      "         1.3053e+00,  1.0271e-01,  4.9739e+00,  2.6493e+00, -1.4281e+00,\n",
      "        -9.9107e-01, -5.5264e+00,  7.4080e+00,  1.3537e-01, -3.5871e+00,\n",
      "         1.4069e+00,  1.9110e-01, -2.4356e+00, -2.6024e-01, -4.1470e+00,\n",
      "         9.0189e-01,  2.6930e+00, -8.2941e-01, -6.2876e+00,  4.4119e+00,\n",
      "        -2.2803e+00, -2.3798e-01, -4.0744e+00,  1.2899e+00,  5.9259e+00,\n",
      "        -1.4491e+00,  4.1115e+00,  7.6507e+00, -5.2266e-01,  8.1478e-01,\n",
      "        -5.9488e-01, -2.8800e+00,  1.6070e+00, -1.3038e+00,  8.4536e+00,\n",
      "         3.1574e+00,  4.3699e+00,  3.4014e+00,  3.5842e-01, -1.3215e+01,\n",
      "        -1.4053e+00, -2.6288e+00,  4.8143e+00,  2.0539e+00,  1.6748e-01,\n",
      "         4.1767e+00, -5.7380e+00, -1.7701e+00,  1.8064e+00, -3.6607e+00,\n",
      "        -5.5449e+00,  2.8341e+00,  4.1512e+00, -4.7526e-01,  6.4209e+00,\n",
      "         7.9021e+00,  2.0874e+00, -1.0135e+00, -4.5659e-01,  3.9279e+00,\n",
      "         5.2623e+00,  1.1785e+01,  5.6953e+00, -6.3919e+00, -1.8736e+00,\n",
      "        -2.2431e-01, -1.1242e+00,  6.3765e+00,  8.5676e+00,  1.8241e+00,\n",
      "         1.3565e+00,  8.2621e+00, -3.7792e+00,  5.2332e-01,  2.7537e+00,\n",
      "         7.2349e+00, -5.9973e+00, -2.3626e+00, -1.7545e+00, -6.0492e+00,\n",
      "        -1.8521e+00,  7.7976e+00, -1.9034e+00, -2.2468e+00, -6.2180e+00,\n",
      "         1.5873e+00, -5.8712e+00,  2.0896e+00, -2.1326e+00,  5.0761e-01,\n",
      "         6.1060e+00, -4.0378e+00,  3.0972e+00, -1.2133e+00,  3.1608e-01,\n",
      "        -5.4708e-01, -1.0551e+01,  3.8208e+00, -4.5960e+00, -4.6720e+00,\n",
      "         4.2589e+00,  2.4463e+00,  8.6115e-01,  3.8327e+00, -1.2704e+00,\n",
      "        -6.9881e+00,  1.6168e+00, -3.6859e-01, -1.1892e+00,  9.8264e-01,\n",
      "        -2.0420e+00,  3.5137e-01, -2.3380e+00,  6.3113e+00,  5.1162e-01,\n",
      "         3.4345e+00,  5.0087e+00, -3.2619e-01,  2.4778e+00,  5.8520e+00,\n",
      "         5.6123e+00,  3.7472e+00,  2.6442e+00, -5.7065e+00, -5.1231e+00,\n",
      "         2.1446e+00, -3.2127e+00, -4.4776e+00, -3.8182e-01,  6.0845e+00,\n",
      "         4.2760e+00, -2.8033e+00, -7.8626e+00,  6.8845e+00, -6.0340e+00,\n",
      "         2.4813e-01,  7.6411e+00, -3.5006e+00, -1.0102e+00, -1.9267e+00,\n",
      "         3.0783e+00,  1.5398e+00, -4.9042e+00,  5.0269e-01,  2.0002e+00,\n",
      "        -1.4566e+00,  7.0904e+00,  2.1972e+00,  7.2551e+00, -7.9671e+00,\n",
      "         3.1286e+00, -5.5992e+00,  5.7037e+00, -6.5521e+00, -1.2914e+01,\n",
      "         3.0881e+00,  1.5450e+00, -2.8072e+00, -1.0299e+01,  5.9666e+00,\n",
      "        -6.3466e+00,  4.7114e+00, -5.0114e-01, -2.1233e+00, -3.1761e+00,\n",
      "        -5.0386e+00,  6.2882e+00,  3.6287e+00])}, 'audio': {'emotion_logits': tensor([-0.1076,  0.1115,  0.2050, -0.5880,  1.1384, -0.1085,  0.2426]), 'personality_scores': tensor([0.5481, 0.4844, 0.4369, 0.5322, 0.5242]), 'last_emo_encoder_features': tensor([-1.3355e+00, -8.5689e-02,  4.6090e-01, -1.8019e-01, -6.8487e-02,\n",
      "        -7.9714e-01,  1.5483e-01, -1.1817e+00, -5.3850e-01,  5.4323e-01,\n",
      "        -2.9138e-01,  1.7660e-01,  3.7927e-01, -8.1957e-02,  2.8967e-01,\n",
      "         3.4342e-01, -3.8540e-01,  4.6886e-01,  7.8265e-02, -1.1122e-01,\n",
      "        -1.7872e-01,  1.6941e-02,  4.7144e-01,  2.6813e-01,  3.0537e-01,\n",
      "         2.3239e-01, -1.1960e-01,  7.3842e-02,  2.6231e-01, -2.6510e-01,\n",
      "        -4.6058e-02, -6.4226e-02, -2.9877e-01,  3.1593e-01, -2.0665e-01,\n",
      "         1.2790e-02, -3.7107e-01, -5.0167e-02,  2.8607e-02,  7.7152e-02,\n",
      "        -4.5779e-01,  2.6486e-01, -1.2267e-01, -1.9282e-01, -2.8726e-01,\n",
      "         1.3352e-01,  1.5981e-01, -2.9530e-02, -7.9643e-02,  3.6666e-02,\n",
      "         8.3654e-02,  4.9430e-01, -1.3549e-01, -3.1343e-01,  8.7429e-01,\n",
      "        -2.2626e-02, -8.8080e-02,  4.9344e-01, -2.0043e-01,  1.5469e-01,\n",
      "         2.8418e-02,  4.4087e-01,  5.0201e-01, -1.2505e-01, -3.3629e-01,\n",
      "         1.2482e-01, -2.8649e-01,  8.7830e-01,  5.3066e-01, -2.7797e-01,\n",
      "        -1.1342e-01, -5.5768e-02,  7.5021e-01, -2.5901e-01, -6.0458e-01,\n",
      "         5.0563e-02, -3.1710e-02, -7.1117e-01, -2.9598e-01,  1.7841e-01,\n",
      "        -1.0658e-01, -2.3616e-01, -2.6212e-03, -2.4693e-01, -4.6913e-01,\n",
      "         4.0845e-01, -3.0498e-01,  6.1779e-02,  4.5943e-01, -4.2238e-01,\n",
      "        -4.7365e-01,  3.4135e-01, -5.7148e-02, -3.6354e-01,  5.4116e-01,\n",
      "         3.3173e-01,  3.6332e-01, -1.1710e-01, -1.0179e+00,  2.4123e-01,\n",
      "         3.9732e-01, -4.3600e-01,  7.1000e-01,  3.4202e-02,  3.2723e-01,\n",
      "         1.7794e-01, -6.2545e-01,  3.9149e-02, -2.2221e-01, -3.1657e-01,\n",
      "         8.5077e-01,  5.1825e-01, -8.3025e-02, -2.2222e-01, -3.4578e-01,\n",
      "        -2.9228e-01, -4.1871e-01,  2.8280e-01, -5.5250e-01, -7.5368e-02,\n",
      "        -6.0876e-02, -3.6595e-01, -3.3945e-01, -3.3391e-02,  2.1097e-01,\n",
      "        -1.8765e-01, -1.5005e-01,  7.2303e-02, -4.1087e-01, -5.7971e-01,\n",
      "        -8.6625e-01,  3.2982e-02, -1.8321e-01, -1.8292e-02, -2.6247e-02,\n",
      "        -2.4729e-01, -1.8026e-01,  5.2362e-02,  4.7769e-01,  9.8055e-02,\n",
      "         9.2599e-01, -2.8133e-01,  1.3465e-02,  3.0121e-01,  7.9329e-01,\n",
      "        -4.4242e-01, -1.2334e-01,  5.0006e-01,  1.2934e-01,  7.4438e-01,\n",
      "         4.0343e-01, -5.5753e-02,  1.8206e-01, -5.0018e-01, -1.8114e-01,\n",
      "         4.7154e-01,  3.3047e-01,  1.0817e-02,  1.7245e-02,  1.4924e-01,\n",
      "         2.7376e-01,  1.1900e-01,  2.3828e-01, -1.5656e-02,  3.4809e-01,\n",
      "        -1.8488e-01,  8.1097e-04,  1.3745e-01, -3.0497e-01,  5.1091e-01,\n",
      "        -5.5081e-02, -2.0967e-02,  4.9937e-01, -2.6562e-01,  4.2074e-01,\n",
      "        -7.1375e-01,  4.0121e-01, -2.0999e-01, -2.2290e-01,  3.1125e-01,\n",
      "         2.3035e-01, -1.0774e-02,  1.4413e-01, -1.6087e-01, -3.7071e-01,\n",
      "        -2.2799e-01,  4.7164e-01,  3.0333e-01,  2.8435e-01, -2.7192e-01,\n",
      "         5.3863e-02, -8.5868e-02, -5.5163e-02, -4.2883e-01,  1.2452e-01,\n",
      "        -3.9853e-01,  4.7589e-01,  4.8190e-01, -3.7574e-01,  1.5178e+00,\n",
      "         4.6646e-01,  2.5487e-01, -1.0455e-01,  2.0801e-01, -4.5364e-01,\n",
      "         5.1163e-01, -6.2002e-02, -8.1130e-03,  9.1308e-02, -2.7466e-01,\n",
      "        -5.4775e-02, -8.6379e-01,  3.4787e-01,  2.9951e-01, -1.3286e-01,\n",
      "        -7.0264e-01,  3.0289e-04, -1.9829e-01, -5.7148e-03,  3.3260e-01,\n",
      "        -4.2053e-01,  1.1689e-01, -4.1891e-01, -3.4421e-01, -3.1856e-02,\n",
      "        -1.6963e-01, -4.7391e-01, -4.9746e-01,  3.7469e-01,  5.7538e-01,\n",
      "        -2.9695e-01, -7.2013e-01,  1.2484e-01,  2.1572e-01,  5.6515e-02,\n",
      "        -4.1075e-01,  7.7875e-01,  6.7799e-01,  4.5674e-03, -4.1912e-01,\n",
      "         5.2129e-01,  7.6064e-01,  3.7258e-01, -5.9563e-01, -5.3276e-01,\n",
      "        -4.9210e-01, -9.0970e-01, -7.5119e-02,  1.7269e-01, -5.2441e-01,\n",
      "         3.8738e-01, -4.1055e-01,  5.6959e-02, -3.2818e-01, -2.5792e-01,\n",
      "         1.2507e-01]), 'last_per_encoder_features': tensor([-0.2641, -0.2310, -0.7558,  0.2488, -0.0789, -0.3536,  0.2219, -0.3574,\n",
      "         1.2963, -0.3156, -0.9599, -0.1017, -0.7655,  0.7691, -0.2863,  0.5405,\n",
      "         0.4290,  0.5654,  0.0710, -0.5230, -0.4857, -0.2002, -0.6224, -0.8489,\n",
      "         0.7986, -0.3274, -0.8192, -0.7866, -0.6279,  0.5334,  0.0350,  0.2803,\n",
      "         0.1434, -0.0461, -0.0235, -0.4954, -0.1844,  0.4977,  0.9379, -0.5396,\n",
      "         0.4824,  0.4107,  1.9974,  0.5315,  0.3247,  0.0282, -0.6316, -0.6064,\n",
      "         0.5380,  0.1842, -0.1066,  0.6489,  0.1048,  0.0164, -0.5976,  0.1883,\n",
      "        -0.2066,  0.2693,  0.5412,  1.0192,  0.4994,  0.1326, -0.0341,  0.8021,\n",
      "        -0.4973, -0.3757,  0.1763, -1.5760,  0.1222,  0.2187, -0.5125, -0.8347,\n",
      "         0.3684,  1.0637, -0.5532, -0.1632,  0.4516,  0.2847, -0.0602, -0.3185,\n",
      "         0.4469,  0.2841, -0.3654,  0.3704,  0.3099, -0.0432, -0.1248, -0.3555,\n",
      "         0.4505, -0.3147,  0.0934,  0.3029, -0.4013,  0.9521,  0.4735, -0.2377,\n",
      "         0.5819,  0.5469, -0.5570,  1.1016,  0.2558,  0.7575, -0.1614,  0.1763,\n",
      "        -0.7110, -0.5784, -0.5939, -0.2927,  0.1854,  1.5980,  0.5500, -0.1313,\n",
      "         0.1461, -0.2330, -0.0644, -0.4123,  0.2928, -0.1154,  0.1881,  0.1703,\n",
      "        -0.3042, -0.4329,  0.4404, -1.1458,  0.0398,  0.8297,  1.1925, -0.7699,\n",
      "         0.5341, -0.3162,  0.7773,  0.1320,  0.5804, -0.2844, -0.3051,  0.3689,\n",
      "        -0.7564,  0.0104, -0.0321,  0.7028,  0.6100,  0.6107,  0.4356, -0.7625,\n",
      "        -0.3859,  0.1661,  0.2778,  0.6780, -0.1868,  0.4524, -0.4024,  0.4588,\n",
      "         1.1699,  0.3063,  0.1276, -0.1493,  0.9676,  0.0106, -0.2752,  0.8598,\n",
      "         1.1187, -0.1397, -0.1536,  0.6003, -0.6024,  0.4441,  0.1409,  0.0336,\n",
      "         0.0286,  0.5055, -0.5980,  0.1287,  0.3479,  0.6116, -0.3302,  0.3260,\n",
      "        -0.4298, -0.8978,  0.3685,  0.8808,  0.7713, -1.2171,  0.7386, -0.0588,\n",
      "         0.3393, -0.2123, -0.1039, -0.2768,  0.9029, -0.2932,  0.4855, -0.2023,\n",
      "        -0.5565, -1.1449,  0.9671, -0.0915, -0.7331,  0.5613, -1.0176,  0.3805,\n",
      "         1.2471,  0.3241, -0.5269, -0.0357, -0.5754,  0.8233, -0.4126,  0.6305,\n",
      "        -0.4254, -0.3425, -1.2499,  0.6832,  0.5735, -0.0742, -0.5933,  0.0673,\n",
      "         1.3720,  0.1959,  0.0391,  0.1923,  0.7568, -0.8194, -0.7462, -0.9003,\n",
      "         0.4604, -0.8845, -0.5075,  0.7874,  0.6474,  0.6273,  0.1853,  0.9412,\n",
      "        -0.6589,  0.7262, -0.2550,  1.3964,  0.0172, -1.7077, -0.8431,  0.0813,\n",
      "         0.6821,  0.6047,  0.5541, -1.9281,  1.2789, -0.0703, -0.9138, -0.2589,\n",
      "        -0.9199, -0.8584, -0.7435,  0.0166,  0.8706, -0.8303, -0.4226, -0.5856])}, 'text': {'emotion_logits': tensor([-1.7925,  0.0654,  0.3438, -0.3643, -1.1636, -0.1055,  2.4729]), 'personality_scores': tensor([0.4693, 0.4101, 0.3828, 0.5089, 0.4350]), 'last_emo_encoder_features': tensor([ 1.3117,  0.1233,  0.1214, -0.6749,  1.3550,  0.2257,  0.5565,  1.1860,\n",
      "        -0.1013, -0.2597, -0.4604,  0.4811,  0.9304,  0.6934, -1.3293, -0.3832,\n",
      "        -0.2334,  0.5496,  0.8171, -0.3050, -2.9424,  0.0758, -0.1881, -0.2041,\n",
      "        -0.9533, -1.3416,  0.1969, -1.0516, -0.7661, -3.4851,  0.9880, -0.4750,\n",
      "         0.5690,  1.2738, -1.3251, -0.5083,  0.9850,  0.9867,  0.3676,  0.6562,\n",
      "        -0.6075, -0.8803,  1.8229,  0.9221,  0.2806,  0.5674,  0.9808,  1.0890,\n",
      "         0.7677, -0.1906,  0.0849, -0.1676,  0.8111, -0.2369, -0.5160,  1.2389,\n",
      "         0.9324, -0.0288,  0.6982,  0.1881,  0.5070, -1.6251, -0.1079, -0.6248,\n",
      "        -0.2233, -2.4996,  1.1034,  0.0304, -0.0927, -0.1810, -0.3401, -0.9445,\n",
      "        -1.8510, -2.1253, -0.6261,  1.2160, -1.8038, -1.3030, -1.5683,  1.6455,\n",
      "        -0.4817,  1.7856, -0.2689, -0.6165, -1.3724, -0.5397,  0.8658, -1.8734,\n",
      "         1.8740,  0.9101, -0.2718, -2.0057, -0.3465, -1.9095, -0.2953, -0.1595,\n",
      "         0.9911, -0.7710, -0.3047, -2.1182, -0.7757, -0.4620, -1.5437, -0.1667,\n",
      "        -0.1403,  0.2478,  0.3537, -0.9259, -2.4161, -0.5158,  2.8783,  0.3477,\n",
      "        -0.4141,  0.5643,  0.7378, -0.4476,  1.1763, -1.1265, -0.0538, -1.0611,\n",
      "         0.4653, -0.8607, -0.1139, -1.3049,  0.2893, -0.5542,  0.4198, -0.0609,\n",
      "        -1.4974,  0.0406, -0.7295, -0.0865, -0.5733, -1.0767, -0.6369,  1.1113,\n",
      "        -0.4406, -1.3484,  1.3967, -0.9612, -0.8654, -0.2711,  1.9411, -1.5866,\n",
      "        -0.0766,  0.1246, -2.0886,  1.6158, -0.0729,  0.7784,  2.0847, -0.6618,\n",
      "         0.3601, -2.0811,  0.2766, -0.2644, -0.0128,  0.1023,  1.0281,  0.0365,\n",
      "         0.1917,  1.7674, -0.2487,  1.2056, -0.3284,  0.4611, -1.6409, -0.7981,\n",
      "        -1.3425, -1.9783, -0.0963, -1.9919,  0.6237, -0.5455, -0.0987, -0.4003,\n",
      "        -1.1158, -0.1506, -1.5576, -0.2314, -0.9786,  2.0852,  1.2366,  1.0729,\n",
      "         0.2970,  0.4120,  0.0085, -0.2069, -0.0069,  0.1019,  0.7239,  0.7537,\n",
      "        -0.1292,  0.1896, -0.4214, -0.8295,  0.5412, -0.3393, -1.3720, -1.1984,\n",
      "         1.8579, -1.0736, -0.0210,  0.7834, -0.6732, -1.0010, -1.3657,  0.6824,\n",
      "        -0.0895,  0.5278,  0.0349,  0.3871,  0.7223,  1.0460, -0.0253, -0.4317,\n",
      "         0.0855, -1.4607, -0.0265, -1.1861,  1.6116,  0.3913, -1.5451,  0.8913,\n",
      "         0.7283,  0.6867,  1.7406,  0.6090, -0.5990,  0.6313, -1.0585,  1.4705,\n",
      "         0.4126, -0.1873, -1.6296,  0.3986, -0.0329,  0.9603,  0.5962, -0.7992,\n",
      "        -0.4637, -1.1269,  0.4375, -2.2573,  0.2851, -1.4316, -0.2300, -0.4036,\n",
      "        -1.3178,  0.5397,  0.8350, -1.7956,  1.2486,  0.8479,  0.4694, -0.4344]), 'last_per_encoder_features': tensor([ 0.5426,  1.0645, -2.0989, -3.1014, -1.3559,  0.5425,  1.0779,  2.7516,\n",
      "        -1.2365,  0.2915,  1.2540, -2.0358, -1.2977,  1.1356, -1.8996, -0.2158,\n",
      "         0.5879, -0.6509, -0.7874, -3.4764,  3.4650,  1.7006, -0.0768,  2.2589,\n",
      "        -0.0663,  1.0481,  2.7645,  1.1218,  0.5240,  4.4635, -0.4183, -2.1453,\n",
      "        -0.1275, -0.8448, -1.4215, -1.4652,  1.1589,  1.8271, -2.8099, -3.1310,\n",
      "         0.6701, -0.2038,  2.6913,  3.0303, -1.2003,  0.6285, -1.4875,  1.8531,\n",
      "         4.8925,  1.5720, -2.8771, -1.5059, -1.4728, -5.5800, -1.6177, -0.5982,\n",
      "        -0.6509, -0.4453,  1.1731, -2.0243,  0.8411, -2.1620,  4.8387, -0.7772,\n",
      "         4.5490, -2.5652, -0.6364, -4.3771, -0.3411, -3.0718, -1.4518, -1.1763,\n",
      "        -2.6802, -0.8860,  1.4294,  2.3022, -1.1359, -0.8532, -0.8563,  5.2685,\n",
      "        -0.7511, -2.3058, -4.0288, -4.6303, -0.2756, -0.3849,  3.0891, -2.1003,\n",
      "        -0.7071, -1.3395,  3.1762,  0.0364, -1.2352, -2.7171, -2.9043, -2.8764,\n",
      "        -2.6795,  0.5879, -0.2619,  2.0823, -3.2588,  0.1975, -0.7615,  0.3763,\n",
      "         1.2806,  2.2529, -3.3445,  0.0143, -2.9520,  2.3400, -1.0386,  2.4104,\n",
      "        -2.6775, -1.4231,  0.2113,  0.8228, -0.3736, -1.7517,  1.9025, -0.6338,\n",
      "        -2.0133, -0.2947,  0.2376,  1.4427,  1.3880,  0.8147,  2.5831,  3.5425,\n",
      "        -3.5206,  1.3838, -0.5941, -0.9153,  2.1187,  3.6365,  0.2208,  3.2290,\n",
      "         0.0278,  1.3414,  0.5766, -0.6114,  2.4437,  1.9112,  2.6598,  3.5696,\n",
      "         2.3405, -0.5312,  0.0389, -2.6998, -3.0750,  2.7324,  2.0325, -0.2043,\n",
      "        -4.5035, -3.3940,  1.6156,  0.1535, -0.8984,  1.9044, -0.6904, -2.1375,\n",
      "         2.1775,  3.5728, -0.7829,  0.7266,  2.6296, -1.0245, -0.2674,  0.7676,\n",
      "         2.7830, -1.4738, -0.3041,  0.1433, -0.2599, -2.1977, -1.9421, -0.5589,\n",
      "        -0.3278,  2.3638, -2.2062, -0.0663,  0.3710, -0.6494,  0.5393,  2.8830,\n",
      "         0.2444, -0.4842, -2.2247, -0.6965, -0.0560, -0.4123, -0.6624, -0.9111,\n",
      "         0.8858, -1.9075, -0.7388, -1.2997,  0.4630,  1.3783, -3.4890,  3.2895,\n",
      "        -1.2114, -2.5037, -0.2853,  2.4516, -1.6238,  0.0358, -1.3388, -0.0261,\n",
      "        -0.6262,  0.6606,  1.3133,  2.1290,  0.7847,  0.1059, -0.5723, -0.9960,\n",
      "         0.2435,  3.1135,  0.3427, -0.1619,  2.0551, -0.6046, -0.5295,  0.2334,\n",
      "         0.1064,  1.3703, -1.7061,  2.5742,  2.7846,  0.9953,  0.1118, -1.5034,\n",
      "        -0.4731,  0.7922, -0.4870,  0.0148,  1.5639,  1.3181, -1.0070,  2.3792,\n",
      "        -0.4095,  0.9049,  0.1947,  2.1153,  0.8144,  1.0136, -3.3148, -4.6253,\n",
      "        -0.8728,  1.1024,  1.7151,  1.6620, -0.7445, -1.5648, -2.8328,  2.9501])}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# путь к твоему .pickle файлу\n",
    "pickle_path = \"../features/fiv2_test_seed_42_subset_size_2_average_features_True_feature_norm_False.pickle\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Посмотреть первый элемент\n",
    "print(\"🔍 Первый элемент:\")\n",
    "item = data[0][\"features\"]\n",
    "print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94645014-a1a1-4c20-8342-d9bf1711a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Шейпы признаков:\n",
      "\n",
      "[BODY]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([1024])\n",
      "  last_per_encoder_features: torch.Size([1024])\n",
      "\n",
      "[FACE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([512])\n",
      "  last_per_encoder_features: torch.Size([512])\n",
      "\n",
      "[SCENE]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([768])\n",
      "  last_per_encoder_features: torch.Size([768])\n",
      "\n",
      "[AUDIO]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n",
      "\n",
      "[TEXT]\n",
      "  emotion_logits: torch.Size([7])\n",
      "  personality_scores: torch.Size([5])\n",
      "  last_emo_encoder_features: torch.Size([256])\n",
      "  last_per_encoder_features: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Дополнительно: распечатать шейпы признаков по модальностям\n",
    "print(\"\\n🔎 Шейпы признаков:\")\n",
    "modalities = item.get(\"features\", {})\n",
    "for mod_name, features in modalities.items():\n",
    "    print(f\"\\n[{mod_name.upper()}]\")\n",
    "    for feat_name, feat_val in features.items():\n",
    "        if isinstance(feat_val, torch.Tensor):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        elif isinstance(feat_val, np.ndarray):\n",
    "            print(f\"  {feat_name}: {feat_val.shape}\")\n",
    "        else:\n",
    "            print(f\"  {feat_name}: not a tensor ({type(feat_val)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ce5167-c20c-45ae-a2b6-83fe00944210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Элементов в списке: 10\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(f\"Элементов в списке: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b54d3cb-f4d8-4c14-b792-c8d6b4b72fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаляем колонку/и: ['Unnamed: 0']\n",
      "Готово. Чистый CSV сохранён сюда: E:/FirstImpressionsV2/dev_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_csv_to_copy(csv_path: str, output_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Удаляем неназванные колонки\n",
    "    unnamed_cols = [col for col in df.columns if col.startswith(\"Unnamed\") or col.strip() == \"\"]\n",
    "    if unnamed_cols:\n",
    "        print(f\"Удаляем колонку/и: {unnamed_cols}\")\n",
    "        df = df.drop(columns=unnamed_cols)\n",
    "\n",
    "    # Чистим расширения в video_name\n",
    "    if 'video_name' not in df.columns:\n",
    "        raise ValueError(\"Файл без 'video_name'. Ты что, решил пошутить?\")\n",
    "    \n",
    "    df['video_name'] = df['video_name'].apply(lambda x: os.path.splitext(str(x))[0])\n",
    "\n",
    "    # Сохраняем в новый файл\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Готово. Чистый CSV сохранён сюда: {output_path}\")\n",
    "    \n",
    "# Пример использования\n",
    "csv_path = \"E:/FirstImpressionsV2/dev_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/dev_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a29e415-3053-4ba2-ac3e-88ee39d105a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаляем колонку/и: ['Unnamed: 0']\n",
      "Готово. Чистый CSV сохранён сюда: E:/FirstImpressionsV2/test_full.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"E:/FirstImpressionsV2/test_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/test_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aae35de-5703-48e0-ad13-8b306d8a2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удаляем колонку/и: ['Unnamed: 0']\n",
      "Готово. Чистый CSV сохранён сюда: E:/FirstImpressionsV2/train_full.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"E:/FirstImpressionsV2/train_FIv2.csv\"\n",
    "output_path = \"E:/FirstImpressionsV2/train_full.csv\"\n",
    "clean_csv_to_copy(csv_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51906294-cbe0-4c0a-83dd-5bb069cbb0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['emo_model.emo_proj.0.weight', 'emo_model.emo_proj.0.bias', 'emo_model.emo_proj.1.weight', 'emo_model.emo_proj.1.bias', 'emo_model.emotion_encoder.0.in_proj.weight', 'emo_model.emotion_encoder.0.in_proj.bias', 'emo_model.emotion_encoder.0.s_B.weight', 'emo_model.emotion_encoder.0.s_B.bias', 'emo_model.emotion_encoder.0.s_C.weight', 'emo_model.emotion_encoder.0.s_C.bias', 'emo_model.emotion_encoder.0.out_proj.weight', 'emo_model.emotion_encoder.0.out_proj.bias', 'emo_model.emotion_encoder.0.norm.weight', 'emo_model.emotion_encoder.0.norm.bias', 'emo_model.emotion_encoder.1.in_proj.weight', 'emo_model.emotion_encoder.1.in_proj.bias', 'emo_model.emotion_encoder.1.s_B.weight', 'emo_model.emotion_encoder.1.s_B.bias', 'emo_model.emotion_encoder.1.s_C.weight', 'emo_model.emotion_encoder.1.s_C.bias', 'emo_model.emotion_encoder.1.out_proj.weight', 'emo_model.emotion_encoder.1.out_proj.bias', 'emo_model.emotion_encoder.1.norm.weight', 'emo_model.emotion_encoder.1.norm.bias', 'emo_model.emotion_encoder.2.in_proj.weight', 'emo_model.emotion_encoder.2.in_proj.bias', 'emo_model.emotion_encoder.2.s_B.weight', 'emo_model.emotion_encoder.2.s_B.bias', 'emo_model.emotion_encoder.2.s_C.weight', 'emo_model.emotion_encoder.2.s_C.bias', 'emo_model.emotion_encoder.2.out_proj.weight', 'emo_model.emotion_encoder.2.out_proj.bias', 'emo_model.emotion_encoder.2.norm.weight', 'emo_model.emotion_encoder.2.norm.bias', 'emo_model.emotion_encoder.3.in_proj.weight', 'emo_model.emotion_encoder.3.in_proj.bias', 'emo_model.emotion_encoder.3.s_B.weight', 'emo_model.emotion_encoder.3.s_B.bias', 'emo_model.emotion_encoder.3.s_C.weight', 'emo_model.emotion_encoder.3.s_C.bias', 'emo_model.emotion_encoder.3.out_proj.weight', 'emo_model.emotion_encoder.3.out_proj.bias', 'emo_model.emotion_encoder.3.norm.weight', 'emo_model.emotion_encoder.3.norm.bias', 'emo_model.emotion_fc_out.0.weight', 'emo_model.emotion_fc_out.0.bias', 'emo_model.emotion_fc_out.1.weight', 'emo_model.emotion_fc_out.1.bias', 'emo_model.emotion_fc_out.4.weight', 'emo_model.emotion_fc_out.4.bias', 'per_model.per_proj.0.weight', 'per_model.per_proj.0.bias', 'per_model.per_proj.1.weight', 'per_model.per_proj.1.bias', 'per_model.personality_encoder.0.self_attention.in_proj_weight', 'per_model.personality_encoder.0.self_attention.in_proj_bias', 'per_model.personality_encoder.0.self_attention.out_proj.weight', 'per_model.personality_encoder.0.self_attention.out_proj.bias', 'per_model.personality_encoder.0.feed_forward.layer_1.weight', 'per_model.personality_encoder.0.feed_forward.layer_1.bias', 'per_model.personality_encoder.0.feed_forward.layer_2.weight', 'per_model.personality_encoder.0.feed_forward.layer_2.bias', 'per_model.personality_encoder.0.add_norm_after_attention.norm.weight', 'per_model.personality_encoder.0.add_norm_after_attention.norm.bias', 'per_model.personality_encoder.0.add_norm_after_ff.norm.weight', 'per_model.personality_encoder.0.add_norm_after_ff.norm.bias', 'per_model.personality_encoder.0.positional_encoding.pe', 'per_model.personality_fc_out.0.weight', 'per_model.personality_fc_out.0.bias', 'per_model.personality_fc_out.1.weight', 'per_model.personality_fc_out.1.bias', 'per_model.personality_fc_out.4.weight', 'per_model.personality_fc_out.4.bias', 'emo_proj.0.weight', 'emo_proj.0.bias', 'emo_proj.1.weight', 'emo_proj.1.bias', 'per_proj.0.weight', 'per_proj.0.bias', 'per_proj.1.weight', 'per_proj.1.bias', 'emotion_to_personality_attn.0.self_attention.in_proj_weight', 'emotion_to_personality_attn.0.self_attention.in_proj_bias', 'emotion_to_personality_attn.0.self_attention.out_proj.weight', 'emotion_to_personality_attn.0.self_attention.out_proj.bias', 'emotion_to_personality_attn.0.feed_forward.layer_1.weight', 'emotion_to_personality_attn.0.feed_forward.layer_1.bias', 'emotion_to_personality_attn.0.feed_forward.layer_2.weight', 'emotion_to_personality_attn.0.feed_forward.layer_2.bias', 'emotion_to_personality_attn.0.add_norm_after_attention.norm.weight', 'emotion_to_personality_attn.0.add_norm_after_attention.norm.bias', 'emotion_to_personality_attn.0.add_norm_after_ff.norm.weight', 'emotion_to_personality_attn.0.add_norm_after_ff.norm.bias', 'emotion_to_personality_attn.0.positional_encoding.pe', 'emotion_to_personality_attn.1.self_attention.in_proj_weight', 'emotion_to_personality_attn.1.self_attention.in_proj_bias', 'emotion_to_personality_attn.1.self_attention.out_proj.weight', 'emotion_to_personality_attn.1.self_attention.out_proj.bias', 'emotion_to_personality_attn.1.feed_forward.layer_1.weight', 'emotion_to_personality_attn.1.feed_forward.layer_1.bias', 'emotion_to_personality_attn.1.feed_forward.layer_2.weight', 'emotion_to_personality_attn.1.feed_forward.layer_2.bias', 'emotion_to_personality_attn.1.add_norm_after_attention.norm.weight', 'emotion_to_personality_attn.1.add_norm_after_attention.norm.bias', 'emotion_to_personality_attn.1.add_norm_after_ff.norm.weight', 'emotion_to_personality_attn.1.add_norm_after_ff.norm.bias', 'emotion_to_personality_attn.1.positional_encoding.pe', 'personality_to_emotion_attn.0.self_attention.in_proj_weight', 'personality_to_emotion_attn.0.self_attention.in_proj_bias', 'personality_to_emotion_attn.0.self_attention.out_proj.weight', 'personality_to_emotion_attn.0.self_attention.out_proj.bias', 'personality_to_emotion_attn.0.feed_forward.layer_1.weight', 'personality_to_emotion_attn.0.feed_forward.layer_1.bias', 'personality_to_emotion_attn.0.feed_forward.layer_2.weight', 'personality_to_emotion_attn.0.feed_forward.layer_2.bias', 'personality_to_emotion_attn.0.add_norm_after_attention.norm.weight', 'personality_to_emotion_attn.0.add_norm_after_attention.norm.bias', 'personality_to_emotion_attn.0.add_norm_after_ff.norm.weight', 'personality_to_emotion_attn.0.add_norm_after_ff.norm.bias', 'personality_to_emotion_attn.0.positional_encoding.pe', 'personality_to_emotion_attn.1.self_attention.in_proj_weight', 'personality_to_emotion_attn.1.self_attention.in_proj_bias', 'personality_to_emotion_attn.1.self_attention.out_proj.weight', 'personality_to_emotion_attn.1.self_attention.out_proj.bias', 'personality_to_emotion_attn.1.feed_forward.layer_1.weight', 'personality_to_emotion_attn.1.feed_forward.layer_1.bias', 'personality_to_emotion_attn.1.feed_forward.layer_2.weight', 'personality_to_emotion_attn.1.feed_forward.layer_2.bias', 'personality_to_emotion_attn.1.add_norm_after_attention.norm.weight', 'personality_to_emotion_attn.1.add_norm_after_attention.norm.bias', 'personality_to_emotion_attn.1.add_norm_after_ff.norm.weight', 'personality_to_emotion_attn.1.add_norm_after_ff.norm.bias', 'personality_to_emotion_attn.1.positional_encoding.pe', 'emotion_personality_fc_out.0.weight', 'emotion_personality_fc_out.0.bias', 'emotion_personality_fc_out.1.weight', 'emotion_personality_fc_out.1.bias', 'emotion_personality_fc_out.4.weight', 'emotion_personality_fc_out.4.bias', 'personality_emotion_fc_out.0.weight', 'personality_emotion_fc_out.0.bias', 'personality_emotion_fc_out.1.weight', 'personality_emotion_fc_out.1.bias', 'personality_emotion_fc_out.4.weight', 'personality_emotion_fc_out.4.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"../modalities/text/checkpoints/Mamba_Transformer_bge-small_fusion.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb29a3-acbc-4709-a56a-057115c001eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
